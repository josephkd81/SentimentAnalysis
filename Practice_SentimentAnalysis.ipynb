{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/keras/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys, os, inspect, codecs\n",
    "import copy\n",
    "import random \n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.contrib.layers.python.layers import linear\n",
    "\n",
    "\n",
    "# token과 target 의 데이터를 처리하여 int(id)와 str(symbol)의 두 형식으로 양방향 처리해주도록 설계된 모듈\n",
    "class Vocab:\n",
    "    def __init__(self, fn, mode='token'):\n",
    "        self.mode = mode \n",
    "        self.token_unk_id = 0 #in 'token.vocab.txt', the id of '_UNK' is 0.\n",
    "        self.token_unk_symbol = '_UNK' #'UNK' means unknown word, a word that doesn't exist the vocabulary set.\n",
    "\n",
    "        self.token_pad_id = 1 #in 'token.vocab.txt', the id of '_PAD' is 1.\n",
    "        self.token_pad_symbol = '_PAD' #'PAD' means non-character on 128 blank.\n",
    "        \n",
    "        self.token_2_id = {} #input from Vocab.load_token_vocab() / output to Vocab.get_id()\n",
    "        self.id_2_token = {} #input from Vocab.load_token_vocab() / output to Vocab.get_symbol()\n",
    "\n",
    "        self.target_2_id = {} #input from Vocab.load_target_vocab() / output to Vocab.get_id()\n",
    "        self.id_2_target = {} #input from Vocab.load_target_vocab() / output to Vocab.get_symbol()\n",
    "\n",
    "        self.target_out_symbol = 'O'\n",
    "        \n",
    "        if mode == 'token'  : self.load_token_vocab(fn)\n",
    "        if mode == 'target' : self.load_target_vocab(fn)\n",
    "\n",
    "    def load_token_vocab(self, fn):\n",
    "        with codecs.open(fn, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n\\r') #.rstrip() 문자열의 오른쪽에 있는 한칸 이상의 공백들을 지우는 문자열 함수\n",
    "                token, id = line.split('\\t') #.split() ()안에 주어진 문자열을 기준으로 나누어 리스트에 넣어 반환함.\n",
    "\n",
    "                id = int(id)\n",
    "\n",
    "                if token == self.token_unk_symbol: #if token is '_UNK', set id to 0.\n",
    "                    self.token_2_id[token] = self.token_unk_id\n",
    "                    continue \n",
    "\n",
    "                if token == self.token_pad_symbol: #if token is '_PAD', set id to 1.\n",
    "                    self.token_2_id[token] = self.token_pad_id\n",
    "                    continue \n",
    "\n",
    "                # other tokens\n",
    "                self.token_2_id[token] = id\n",
    "                self.id_2_token[id] = token \n",
    "\n",
    "    def load_target_vocab(self, fn):\n",
    "        with codecs.open(fn, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n\\r')\n",
    "                target, id = line.split('\\t')\n",
    "                id = int(id)\n",
    "\n",
    "                self.target_2_id[target] = id\n",
    "                self.id_2_target[id] = target\n",
    "\n",
    "    def get_id(self, symbol): #input from load_token_vocab() and load_target_vocab()\n",
    "        if self.mode == 'token':\n",
    "            return self.token_2_id.get(symbol, self.token_unk_id)\n",
    "\n",
    "        if self.mode == 'target':\n",
    "            return self.target_2_id.get(symbol)\n",
    "\n",
    "    def get_symbol(self, id): #input from load_token_vocab() and load_target_vocab()\n",
    "        if self.mode == 'token':\n",
    "            return self.id_2_token.get(id)\n",
    "\n",
    "        if self.mode == 'target':\n",
    "            return self.id_2_target.get(id)\n",
    "\n",
    "    def get_num_tokens(self):\n",
    "        if self.mode == 'token': return len(self.token_2_id)\n",
    "        return None \n",
    "\n",
    "    def get_num_targets(self):\n",
    "        if self.mode == 'target': return len(self.target_2_id)\n",
    "        return None \n",
    "\n",
    "    def get_token_pad_id(self):   return self.token_pad_id\n",
    "    def get_target_null_id(self): return self.get_id(self.target_out_symbol)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    " \n",
    "class N21Item:\n",
    "    def __init__(self, target, text):\n",
    "        self.target = target\n",
    "        self.text   = text\n",
    "\n",
    "        self.target_id   = None\n",
    "        self.token_ids   = None  # it should be array \n",
    "\n",
    "    def set_id(self, target_id, token_ids):\n",
    "        self.target_id = target_id\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def get_tokens(self):\n",
    "        # currently, only support 'character' \n",
    "        return list(self.text) \n",
    "\n",
    "    \n",
    "# txt file 또는 prediction()에서 개별 입력된 sentence 를 읽어와서 target, text 의 각각의 개체로 반환하여 data라는 리스트로 반환하는 모듈.\n",
    "class N21TextData:\n",
    "    def __init__(self, src=None, mode='file'):  # mode = 'file' | 'sentence'\n",
    "        self.data = []\n",
    "        \n",
    "        if mode == 'file':      self.load_text_file_data(src)\n",
    "        if mode == 'sentence':  self.load_text_data(src)\n",
    "\n",
    "    def add_to_data(self, target, text):\n",
    "        # normalize\n",
    "        target = target.upper()\n",
    "        text   = text.upper()\n",
    "        self.data.append( N21Item(target, text) )\n",
    "\n",
    "    def load_text_data(self, line):\n",
    "        # mode = 'sentence'\n",
    "        # format of line : \"TAG  \\t  SENTENCE\"\n",
    "        line = line.rstrip('\\n\\r')\n",
    "        target, text = line.split('\\t')\n",
    "        self.add_to_data(target, text)\n",
    "\n",
    "    def load_text_file_data(self, fn):\n",
    "        # mode = 'file' \n",
    "        with codecs.open(fn, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n\\r')\n",
    "                target, text = line.split('\\t')\n",
    "\n",
    "                self.add_to_data(target, text)\n",
    "                \n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, id_data, batch_size, num_steps, target_num_step=None, pad_id=1, target_null_id=0, deterministic=False):\n",
    "        self.data            = id_data       # it should be id-based data\n",
    "        \n",
    "        self.token_pad_id    = pad_id\n",
    "        self.target_null_id  = target_null_id\n",
    "\n",
    "        self.batch_size      = batch_size\n",
    "        self.num_steps       = num_steps\n",
    "\n",
    "        self.src_num_steps   = num_steps\n",
    "        self.tar_num_steps   = target_num_step  # for sequence to sequence dataset\n",
    "\n",
    "        self.deterministic   = deterministic  # if deterministic is True, data is shuffled and retrieved\n",
    "        self.iterator           = self.iterate_forever() #during trian, shuffle index\n",
    "        self.predict_iterator   = self.iterate_once() #during test, don't shuffle index\n",
    "\n",
    "        self.epoch = 0 \n",
    "\n",
    "    def get_num_examples(self): return len( self.data ) \n",
    "    def get_epoch_num(self): return self.epoch \n",
    "\n",
    "    def _iterate(self, index_gen, batch_size, max_len):\n",
    "        \"\"\" Abstraction method for _iterate function\"\"\"\n",
    "        raise NotImplementedError(\"Abstract method.\".format( self.run.__name__))\n",
    "    \n",
    "    # for training\n",
    "    def iterate_forever(self):\n",
    "        def index_stream():\n",
    "            # yield data index \n",
    "            self.indexs = list( range( self.get_num_examples() ) )\n",
    "            while True:\n",
    "                self.epoch += 1 \n",
    "                if not self.deterministic:\n",
    "                    random.shuffle( self.indexs ) \n",
    "                for index in self.indexs:\n",
    "                    yield index \n",
    "\n",
    "        for a_data in self._iterate(index_stream()):\n",
    "            yield a_data\n",
    "\n",
    "    # for testing\n",
    "    def iterate_once(self):\n",
    "        def index_stream():\n",
    "            # yield data index \n",
    "            self.indexs = list( range( self.get_num_examples() ) )\n",
    "            for index in self.indexs:\n",
    "                yield index \n",
    "\n",
    "        for a_data in self._iterate(index_stream()):\n",
    "            yield a_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through Vocab class, convert text data to id_data which is tokenized to int.\n",
    "\n",
    "class N21Converter:\n",
    "\n",
    "    @staticmethod\n",
    "    def convert(txt_data, target_vocab, token_vocab):\n",
    "        # txt_data : it should be N21TextData\n",
    "        # target_vocab    : it should be Vocab\n",
    "        # token_vocab    : it should be Vocab\n",
    "\n",
    "        id_data = []  # it should be list of N21Item\n",
    "\n",
    "        for item in txt_data.data:\n",
    "            target_id = target_vocab.get_id(item.target)\n",
    "            text_tokens = item.get_tokens()\n",
    "\n",
    "            token_ids = [ token_vocab.get_id(token) for token in text_tokens ] # for each token\n",
    "\n",
    "            new_item = copy.deepcopy(item)\n",
    "            new_item.set_id(target_id,token_ids)\n",
    "\n",
    "            id_data.append( new_item )\n",
    "        return id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for sentiment dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def _iterate(self, index_gen):\n",
    "        B = self.batch_size\n",
    "        N = self.num_steps\n",
    "\n",
    "        # vectorize id data\n",
    "        sentiment  = np.zeros([B],    np.int64)  \n",
    "        token      = np.zeros([B, N], np.int64)\n",
    "        weight     = np.zeros([B, N], np.int64)\n",
    "\n",
    "        while True:\n",
    "            sentiment[:]  = 0\n",
    "            token[:]      = 0\n",
    "            weight[:]     = 0\n",
    "\n",
    "            for b in range(B):\n",
    "                try:\n",
    "                    while True:\n",
    "                        index = next(index_gen)\n",
    "                        _num_steps = len( self.data[index].token_ids )\n",
    "                        if _num_steps <= N: break \n",
    "\n",
    "                    _sentiment_id = copy.deepcopy( self.data[index].target_id )\n",
    "                    _token_ids    = copy.deepcopy( self.data[index].token_ids )\n",
    "\n",
    "                    # fill pad for weight\n",
    "                    _weight_ids   = [0] * self.num_steps\n",
    "                    for _idx, _ in enumerate(_token_ids): _weight_ids[_idx] = 1\n",
    "\n",
    "                    # fill pad to token\n",
    "                    _token_ids += [self.token_pad_id] * ( self.num_steps - len( _token_ids ) ) \n",
    "\n",
    "                    # output\n",
    "                    sentiment[b] = -1 if _sentiment_id is None else _sentiment_id\n",
    "\n",
    "                    # input\n",
    "                    token[b]  = _token_ids\n",
    "                    weight[b] = _weight_ids\n",
    "\n",
    "                except StopIteration:\n",
    "                    pass\n",
    "            if not np.any(weight):\n",
    "                return\n",
    "            yield sentiment, token, weight  # tuple for (target, input)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    # vocab loader\n",
    "    token_vocab_fn  = os.path.join( os.path.dirname('__file__'), 'data', 'token.vocab.txt')\n",
    "    #__file__ 은 현재 프로글매의 파일 위치를 알아냄.\n",
    "    #os.path.dirname()은 입력된 파일의 디렉토리 이름을 알아냄.\n",
    "    #os.path.join() 은 디렉토리와 파일명을 이어주는 함수.\n",
    "    token_vocab     = Vocab(token_vocab_fn, mode='token')\n",
    "    target_vocab_fn = os.path.join( os.path.dirname('__file__'), 'data', 'target.vocab.txt')\n",
    "    target_vocab    = Vocab(target_vocab_fn, mode='target')\n",
    "\n",
    "    # load train data \n",
    "    #train_data_fn  = os.path.join( os.path.dirname('__file__'), 'data', 'train.sent_data.txt')\n",
    "    train_data_fn  = os.path.join( os.path.dirname('__file__'), 'data', 'train_data.txt')\n",
    "    train_txt_data = N21TextData(train_data_fn)\n",
    "\n",
    "    # convert text data to id data\n",
    "    train_id_data  = N21Converter.convert(train_txt_data, target_vocab, token_vocab)\n",
    "    \n",
    "    return train_id_data, token_vocab, target_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_data, token_vocab, target_vocab = load_data()\n",
    "num_vocabs       = token_vocab.get_num_tokens() #Vocab.get_num_tokens()\n",
    "num_target_class = target_vocab.get_num_targets() #Vocab.get_num_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._items = {}\n",
    "        for k, v in kwargs.items():\n",
    "            self._set(k, v)\n",
    "\n",
    "    def _set(self, k, v):\n",
    "        self._items[k] = v\n",
    "        setattr(self, k, v)\n",
    "\n",
    "    def parse(self, str_value):\n",
    "        hps = HParams(**self._items)\n",
    "        for entry in str_value.strip().split(\",\"):\n",
    "            entry = entry.strip()\n",
    "            if not entry:\n",
    "                continue\n",
    "            key, sep, value = entry.partition(\"=\")\n",
    "            if not sep:\n",
    "                raise ValueError(\"Unable to parse: %s\" % entry)\n",
    "            default_value = hps._items[key]\n",
    "            if isinstance(default_value, bool):\n",
    "                hps._set(key, value.lower() == \"true\")\n",
    "            elif isinstance(default_value, int):\n",
    "                hps._set(key, int(value))\n",
    "            elif isinstance(default_value, float):\n",
    "                hps._set(key, float(value))\n",
    "            else:\n",
    "                hps._set(key, value)\n",
    "        return hps\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            self._set(k, v)\n",
    "\n",
    "    def show(self):\n",
    "        for k, v in self._items.items():\n",
    "            print( u'{} : {}'.format(k,v) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy\n",
    "\n",
    "def freeze_graph(model_dir, output_node_names, frozen_graph_name):\n",
    "    \"\"\"Extract the sub graph defined by the output nodes and convert \n",
    "    all its variables into constant \n",
    "    Args:\n",
    "        model_dir: the root folder containing the checkpoint state file\n",
    "        output_node_names: a string, containing all the output node's names, \n",
    "                            comma separated\n",
    "\n",
    "    thiso code is from : https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            \"directory: %s\" % model_dir)\n",
    "\n",
    "    if not output_node_names:\n",
    "        print(\"You need to supply the name of a node to --output_node_names.\")\n",
    "        return -1\n",
    "\n",
    "    # We retrieve our checkpoint fullpath\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    \n",
    "    # We precise the file fullname of our freezed graph\n",
    "    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = os.path.join(absolute_model_dir, frozen_graph_name)\n",
    "\n",
    "    # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "    clear_devices = True\n",
    "\n",
    "    # We start a session using a temporary fresh Graph\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        # We import the meta graph in the current default Graph\n",
    "        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "        # We restore the weights\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "\n",
    "        # We use a built-in TF helper to export variables to constants\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess, # The session is used to retrieve the weights\n",
    "            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
    "        ) \n",
    "\n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "\n",
    "    return output_graph_def\n",
    "\n",
    "\n",
    "def load_graph(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "    #\n",
    "    # this code is from https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc\n",
    "    #\n",
    "\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Then, we import the graph_def into a new Graph and returns it \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # The name var will prefix every op/nodes in your graph\n",
    "        # Since we load everything in a new graph, this is not needed\n",
    "        tf.import_graph_def(graph_def, name=\"prefix\")\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentimentAnalysis():\n",
    "    def __init__(self, hps, mode=\"train\"):\n",
    "        self.hps = hps\n",
    "        self.x = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_tokens\")\n",
    "        self.y = tf.placeholder(tf.int32,   [None], name=\"pl_target\")\n",
    "        self.w = tf.placeholder(tf.float32, [None, hps.num_steps], name=\"pl_weight\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"pl_keep_prob\")\n",
    "\n",
    "        def _embedding(x):\n",
    "            # character embedding \n",
    "            shape       = [hps.vocab_size, hps.emb_size]\n",
    "            initializer = tf.initializers.variance_scaling(distribution=\"uniform\", dtype=tf.float32)\n",
    "            emb_mat     = tf.get_variable(\"emb\", shape, initializer=initializer, dtype=tf.float32)\n",
    "            input_emb   = tf.nn.embedding_lookup(emb_mat, x)   # [batch_size, sent_len, emb_dim]\n",
    "\n",
    "            # split input_emb -> num_steps\n",
    "            step_inputs = tf.unstack(input_emb, axis=1)\n",
    "            return step_inputs\n",
    "\n",
    "        def _sequence_dropout(step_inputs, keep_prob):\n",
    "            # apply dropout to each input\n",
    "            # input : a list of input tensor which shape is [None, input_dim]\n",
    "            with tf.name_scope('sequence_dropout') as scope:\n",
    "                step_outputs = []\n",
    "                for t, input in enumerate(step_inputs):\n",
    "                    step_outputs.append( tf.nn.dropout(input, keep_prob) )\n",
    "            return step_outputs\n",
    "\n",
    "        def sequence_encoding_n21_rnn(step_inputs, cell_size, scope_name):\n",
    "            # rnn based N21 encoding (GRU)\n",
    "            step_inputs = list( reversed( step_inputs ) )\n",
    "            f_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=None)\n",
    "            _inputs = tf.stack(step_inputs, axis=1)\n",
    "            step_outputs, final_state = tf.contrib.rnn.static_rnn(f_rnn_cell,\n",
    "                                                                  step_inputs,\n",
    "                                                                  dtype=tf.float32,\n",
    "                                                                  scope=scope_name)\n",
    "            \n",
    "            out = step_outputs[-1]\n",
    "            return out\n",
    "\n",
    "        def _to_class(input, num_class):\n",
    "            out = linear(input, num_class, scope=\"Rnn2Sentiment\") # out = [batch_size, 4]\n",
    "            return out\n",
    "\n",
    "        def _loss(out, ref):\n",
    "            # out : [batch_size, num_class] float - unscaled logits\n",
    "            # ref : [batch_size] integer\n",
    "            # calculate loss function using cross-entropy\n",
    "            # sparce_softmax_cross_entropy_with_logits()는 소프트맥스 활성화 함수를 적용한 다음 크로스 엔트로피를 계산한 것과 같음.\n",
    "            # 추가적으로 크로스엔트로피 함수는 로짓이 커지면 부동소수점 반올림 오차로 소프트맥스 출력이 0 또는 1이 되는 문제가 있는데, \n",
    "            # 이는 음의 무한대가 되는 log(0)이 공식에 포함되어 있기 때문. 이를 작은 양수 e에 대해 log(e)를 적용함으로 문제를 해결함.\n",
    "            batch_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=out, labels=ref, name=\"sentiment_loss\") # [batch_size]\n",
    "            loss = tf.reduce_mean(batch_loss)\n",
    "            return loss\n",
    "        \n",
    "        seq_length    = tf.reduce_sum(self.w, 1) # [batch_size]\n",
    "\n",
    "        step_inputs   = _embedding(self.x)\n",
    "        step_inputs   = _sequence_dropout(step_inputs, self.keep_prob)\n",
    "        sent_encoding = sequence_encoding_n21_rnn(step_inputs, hps.enc_dim, scope_name=\"encoder\")\n",
    "        out           = _to_class(sent_encoding, hps.num_target_class)\n",
    "        loss          = _loss(out, self.y) \n",
    "\n",
    "        out_probs     = tf.nn.softmax(out, name=\"out_probs\")\n",
    "        out_pred      = tf.argmax(out_probs, 1, name=\"out_pred\")\n",
    "\n",
    "        self.loss      = loss\n",
    "        self.out_probs = out_probs\n",
    "        self.out_pred  = out_pred\n",
    "\n",
    "        self.global_step = tf.get_variable(\"global_step\", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            optimizer       = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "            self.train_op   = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        else:\n",
    "            self.train_op = tf.no_op()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_hparams():\n",
    "        return HParams(\n",
    "            learning_rate     = 0.001,\n",
    "            keep_prob         = 0.5,\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "def train(train_id_data, num_vocabs, num_taget_class):\n",
    "    #\n",
    "    # train sentiment analysis using given train_id_data\n",
    "    #\n",
    "    max_epoch = 2000\n",
    "    model_dir = \"./trained_models\"\n",
    "    hps = SentimentAnalysis.get_default_hparams()\n",
    "    hps.update(\n",
    "                    batch_size= 100,\n",
    "                    num_steps = 70,\n",
    "                    emb_size  = 50,\n",
    "                    enc_dim   = 100,\n",
    "                    vocab_size=num_vocabs,\n",
    "                    num_target_class=num_taget_class\n",
    "               )\n",
    "\n",
    "    with tf.variable_scope(\"model\"):\n",
    "        model = SentimentAnalysis(hps, \"train\")\n",
    "\n",
    "    #create 'supervisor' to review training process\n",
    "    #supervisor manage initialize of session, restore a model from checkpoint, \n",
    "    #and close the program when error is raise or operation is done.\n",
    "    #reference: https://www.tensorflow.org/api_docs/python/tf/train/Supervisor\n",
    "    sv = tf.train.Supervisor(is_chief=True,\n",
    "                             logdir=model_dir,\n",
    "                             summary_op=None,  \n",
    "                             global_step=model.global_step)\n",
    "\n",
    "    # tf assign compatible operators for gpu and cpu \n",
    "    tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "    with sv.managed_session(config=tf_config) as sess:\n",
    "        local_step       = 0\n",
    "        prev_global_step = sess.run(model.global_step)\n",
    "\n",
    "        train_data_set = SentimentDataset(train_id_data, hps.batch_size, hps.num_steps)\n",
    "        losses = []\n",
    "        while not sv.should_stop():\n",
    "            fetches = [model.global_step, model.loss, model.train_op]\n",
    "            a_batch_data = next( train_data_set.iterator )\n",
    "            y, x, w = a_batch_data\n",
    "            fetched = sess.run(fetches, {\n",
    "                                            model.x: x, \n",
    "                                            model.y: y, \n",
    "                                            model.w: w,\n",
    "\n",
    "                                            model.keep_prob: hps.keep_prob,\n",
    "                                        }\n",
    "                              )\n",
    "\n",
    "            local_step += 1\n",
    "\n",
    "            _global_step = fetched[0]\n",
    "            _loss        = fetched[1]\n",
    "            losses.append( _loss )\n",
    "            if local_step < 10 or local_step % 10 == 0:\n",
    "                epoch = train_data_set.get_epoch_num()\n",
    "                print(\"Epoch = {:3d} Step = {:7d} loss = {:5.3f}\".format(epoch, _global_step, np.mean(losses)) )\n",
    "                _loss = []                \n",
    "                if epoch >= max_epoch : break \n",
    "\n",
    "        print(\"Training is done.\")\n",
    "    sv.stop()\n",
    "\n",
    "    # model.out_pred, model.out_probs\n",
    "    freeze_graph(model_dir, \"model/out_pred,model/out_probs\", \"frozen_graph.tf.pb\") ## freeze graph with params to probobuf format\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "\n",
    "def predict(token_vocab, target_vocab, sent): # mode = 'file' | 'sentence'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force to use cpu only (prediction)\n",
    "    model_dir = \"./trained_models\"\n",
    "\n",
    "    # prepare sentence converting\n",
    "    # to make raw sentence to id data easily\n",
    "    in_sent       = '{}\\t{}'.format('___DUMMY_CLASS___', sent)\n",
    "    pred_data     = N21TextData(in_sent, mode='sentence')       \n",
    "    pred_id_data  = N21Converter.convert(pred_data, target_vocab, token_vocab)\n",
    "    pred_data_set = SentimentDataset(pred_id_data, 1, 70)\n",
    "\n",
    "    #\n",
    "    a_batch_data = next(pred_data_set.predict_iterator) # a result\n",
    "    b_sentiment_id, b_token_ids, b_weight = a_batch_data\n",
    "\n",
    "    # Restore graph\n",
    "    # note that frozen_graph.tf.pb contains graph definition with parameter values in binary format\n",
    "    _graph_fn =  os.path.join(model_dir, 'frozen_graph.tf.pb')\n",
    "    with tf.gfile.GFile(_graph_fn, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # to check load graph\n",
    "        #for n in tf.get_default_graph().as_graph_def().node: print(n.name)\n",
    "\n",
    "        # make interface for input\n",
    "        pl_token     = graph.get_tensor_by_name('import/model/pl_tokens:0')\n",
    "        pl_keep_prob = graph.get_tensor_by_name('import/model/pl_keep_prob:0')\n",
    "\n",
    "        # make interface for output\n",
    "        out_pred  = graph.get_tensor_by_name('import/model/out_pred:0')\n",
    "        out_probs = graph.get_tensor_by_name('import/model/out_probs:0')\n",
    "        \n",
    "\n",
    "        # predict sentence \n",
    "        b_best_pred_index, b_pred_probs = sess.run([out_pred, out_probs], feed_dict={\n",
    "                                                                                        pl_token : b_token_ids,\n",
    "                                                                                        pl_keep_prob : 1.0,\n",
    "                                                                                    }\n",
    "                                          )\n",
    "\n",
    "        best_pred_index = b_best_pred_index[0]\n",
    "        pred_probs = b_pred_probs[0]\n",
    "\n",
    "        best_target_class = target_vocab.get_symbol(best_pred_index)\n",
    "        print( 'pred_target:', best_target_class,'pred_probs:', pred_probs[best_pred_index] )\n",
    "\n",
    "    return best_target_class, pred_probs[best_pred_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-ad453c3cf4b4>:28: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models/model.ckpt\n",
      "INFO:tensorflow:model/global_step/sec: 0\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "Epoch =   1 Step =       1 loss = 1.386\n",
      "Epoch =   1 Step =       2 loss = 1.379\n",
      "Epoch =   1 Step =       3 loss = 1.374\n",
      "Epoch =   1 Step =       4 loss = 1.365\n",
      "Epoch =   1 Step =       5 loss = 1.362\n",
      "Epoch =   1 Step =       6 loss = 1.349\n",
      "Epoch =   1 Step =       7 loss = 1.335\n",
      "Epoch =   1 Step =       8 loss = 1.326\n",
      "Epoch =   1 Step =       9 loss = 1.327\n",
      "Epoch =   2 Step =      10 loss = 1.323\n",
      "Epoch =   3 Step =      20 loss = 1.243\n",
      "Epoch =   4 Step =      30 loss = 1.189\n",
      "Epoch =   5 Step =      40 loss = 1.157\n",
      "Epoch =   6 Step =      50 loss = 1.131\n",
      "Epoch =   7 Step =      60 loss = 1.103\n",
      "Epoch =   8 Step =      70 loss = 1.072\n",
      "Epoch =   9 Step =      80 loss = 1.035\n",
      "Epoch =  10 Step =      90 loss = 1.000\n",
      "Epoch =  12 Step =     100 loss = 0.970\n",
      "Epoch =  13 Step =     110 loss = 0.944\n",
      "Epoch =  14 Step =     120 loss = 0.916\n",
      "Epoch =  15 Step =     130 loss = 0.891\n",
      "Epoch =  16 Step =     140 loss = 0.866\n",
      "Epoch =  17 Step =     150 loss = 0.841\n",
      "Epoch =  18 Step =     160 loss = 0.815\n",
      "Epoch =  19 Step =     170 loss = 0.790\n",
      "Epoch =  20 Step =     180 loss = 0.764\n",
      "Epoch =  22 Step =     190 loss = 0.741\n",
      "Epoch =  23 Step =     200 loss = 0.721\n",
      "Epoch =  24 Step =     210 loss = 0.701\n",
      "Epoch =  25 Step =     220 loss = 0.680\n",
      "Epoch =  26 Step =     230 loss = 0.662\n",
      "Epoch =  27 Step =     240 loss = 0.644\n",
      "Epoch =  28 Step =     250 loss = 0.626\n",
      "Epoch =  29 Step =     260 loss = 0.609\n",
      "Epoch =  30 Step =     270 loss = 0.594\n",
      "Epoch =  32 Step =     280 loss = 0.579\n",
      "Epoch =  33 Step =     290 loss = 0.565\n",
      "Epoch =  34 Step =     300 loss = 0.550\n",
      "Epoch =  35 Step =     310 loss = 0.537\n",
      "Epoch =  36 Step =     320 loss = 0.524\n",
      "Epoch =  37 Step =     330 loss = 0.513\n",
      "Epoch =  38 Step =     340 loss = 0.501\n",
      "Epoch =  39 Step =     350 loss = 0.490\n",
      "Epoch =  40 Step =     360 loss = 0.479\n",
      "Epoch =  42 Step =     370 loss = 0.469\n",
      "Epoch =  43 Step =     380 loss = 0.460\n",
      "Epoch =  44 Step =     390 loss = 0.451\n",
      "Epoch =  45 Step =     400 loss = 0.442\n",
      "Epoch =  46 Step =     410 loss = 0.434\n",
      "Epoch =  47 Step =     420 loss = 0.425\n",
      "Epoch =  48 Step =     430 loss = 0.417\n",
      "Epoch =  49 Step =     440 loss = 0.409\n",
      "Epoch =  50 Step =     450 loss = 0.402\n",
      "Epoch =  52 Step =     460 loss = 0.395\n",
      "Epoch =  53 Step =     470 loss = 0.389\n",
      "Epoch =  54 Step =     480 loss = 0.382\n",
      "Epoch =  55 Step =     490 loss = 0.375\n",
      "Epoch =  56 Step =     500 loss = 0.370\n",
      "Epoch =  57 Step =     510 loss = 0.364\n",
      "Epoch =  58 Step =     520 loss = 0.359\n",
      "Epoch =  59 Step =     530 loss = 0.354\n",
      "Epoch =  60 Step =     540 loss = 0.348\n",
      "Epoch =  62 Step =     550 loss = 0.343\n",
      "Epoch =  63 Step =     560 loss = 0.338\n",
      "Epoch =  64 Step =     570 loss = 0.333\n",
      "Epoch =  65 Step =     580 loss = 0.328\n",
      "Epoch =  66 Step =     590 loss = 0.323\n",
      "Epoch =  67 Step =     600 loss = 0.319\n",
      "Epoch =  68 Step =     610 loss = 0.315\n",
      "Epoch =  69 Step =     620 loss = 0.311\n",
      "Epoch =  70 Step =     630 loss = 0.307\n",
      "Epoch =  72 Step =     640 loss = 0.303\n",
      "Epoch =  73 Step =     650 loss = 0.298\n",
      "Epoch =  74 Step =     660 loss = 0.295\n",
      "Epoch =  75 Step =     670 loss = 0.291\n",
      "Epoch =  76 Step =     680 loss = 0.287\n",
      "Epoch =  77 Step =     690 loss = 0.284\n",
      "Epoch =  78 Step =     700 loss = 0.280\n",
      "Epoch =  79 Step =     710 loss = 0.277\n",
      "Epoch =  80 Step =     720 loss = 0.273\n",
      "Epoch =  82 Step =     730 loss = 0.270\n",
      "Epoch =  83 Step =     740 loss = 0.267\n",
      "Epoch =  84 Step =     750 loss = 0.264\n",
      "Epoch =  85 Step =     760 loss = 0.261\n",
      "Epoch =  86 Step =     770 loss = 0.258\n",
      "Epoch =  87 Step =     780 loss = 0.255\n",
      "Epoch =  88 Step =     790 loss = 0.252\n",
      "Epoch =  89 Step =     800 loss = 0.250\n",
      "Epoch =  90 Step =     810 loss = 0.247\n",
      "Epoch =  92 Step =     820 loss = 0.244\n",
      "Epoch =  93 Step =     830 loss = 0.242\n",
      "Epoch =  94 Step =     840 loss = 0.239\n",
      "Epoch =  95 Step =     850 loss = 0.237\n",
      "Epoch =  96 Step =     860 loss = 0.235\n",
      "Epoch =  97 Step =     870 loss = 0.232\n",
      "Epoch =  98 Step =     880 loss = 0.230\n",
      "Epoch =  99 Step =     890 loss = 0.228\n",
      "Epoch = 100 Step =     900 loss = 0.226\n",
      "Epoch = 102 Step =     910 loss = 0.224\n",
      "Epoch = 103 Step =     920 loss = 0.222\n",
      "Epoch = 104 Step =     930 loss = 0.220\n",
      "Epoch = 105 Step =     940 loss = 0.218\n",
      "Epoch = 106 Step =     950 loss = 0.216\n",
      "Epoch = 107 Step =     960 loss = 0.214\n",
      "Epoch = 108 Step =     970 loss = 0.212\n",
      "Epoch = 109 Step =     980 loss = 0.210\n",
      "Epoch = 110 Step =     990 loss = 0.208\n",
      "Epoch = 112 Step =    1000 loss = 0.206\n",
      "Epoch = 113 Step =    1010 loss = 0.204\n",
      "Epoch = 114 Step =    1020 loss = 0.202\n",
      "Epoch = 115 Step =    1030 loss = 0.201\n",
      "Epoch = 116 Step =    1040 loss = 0.199\n",
      "Epoch = 117 Step =    1050 loss = 0.197\n",
      "Epoch = 118 Step =    1060 loss = 0.196\n",
      "Epoch = 119 Step =    1070 loss = 0.194\n",
      "Epoch = 120 Step =    1080 loss = 0.193\n",
      "Epoch = 122 Step =    1090 loss = 0.191\n",
      "Epoch = 123 Step =    1100 loss = 0.189\n",
      "Epoch = 124 Step =    1110 loss = 0.188\n",
      "Epoch = 125 Step =    1120 loss = 0.186\n",
      "Epoch = 126 Step =    1130 loss = 0.185\n",
      "Epoch = 127 Step =    1140 loss = 0.183\n",
      "Epoch = 128 Step =    1150 loss = 0.182\n",
      "Epoch = 129 Step =    1160 loss = 0.181\n",
      "Epoch = 130 Step =    1170 loss = 0.179\n",
      "Epoch = 132 Step =    1180 loss = 0.178\n",
      "Epoch = 133 Step =    1190 loss = 0.177\n",
      "Epoch = 134 Step =    1200 loss = 0.175\n",
      "Epoch = 135 Step =    1210 loss = 0.174\n",
      "Epoch = 136 Step =    1220 loss = 0.173\n",
      "Epoch = 137 Step =    1230 loss = 0.172\n",
      "Epoch = 138 Step =    1240 loss = 0.171\n",
      "Epoch = 139 Step =    1250 loss = 0.169\n",
      "Epoch = 140 Step =    1260 loss = 0.168\n",
      "Epoch = 142 Step =    1270 loss = 0.167\n",
      "Epoch = 143 Step =    1280 loss = 0.166\n",
      "Epoch = 144 Step =    1290 loss = 0.165\n",
      "Epoch = 145 Step =    1300 loss = 0.164\n",
      "Epoch = 146 Step =    1310 loss = 0.163\n",
      "Epoch = 147 Step =    1320 loss = 0.162\n",
      "Epoch = 148 Step =    1330 loss = 0.160\n",
      "Epoch = 149 Step =    1340 loss = 0.159\n",
      "Epoch = 150 Step =    1350 loss = 0.158\n",
      "Epoch = 152 Step =    1360 loss = 0.157\n",
      "Epoch = 153 Step =    1370 loss = 0.156\n",
      "Epoch = 154 Step =    1380 loss = 0.155\n",
      "Epoch = 155 Step =    1390 loss = 0.154\n",
      "Epoch = 156 Step =    1400 loss = 0.153\n",
      "Epoch = 157 Step =    1410 loss = 0.152\n",
      "Epoch = 158 Step =    1420 loss = 0.151\n",
      "Epoch = 159 Step =    1430 loss = 0.150\n",
      "Epoch = 160 Step =    1440 loss = 0.150\n",
      "Epoch = 162 Step =    1450 loss = 0.149\n",
      "Epoch = 163 Step =    1460 loss = 0.148\n",
      "Epoch = 164 Step =    1470 loss = 0.147\n",
      "Epoch = 165 Step =    1480 loss = 0.146\n",
      "Epoch = 166 Step =    1490 loss = 0.145\n",
      "Epoch = 167 Step =    1500 loss = 0.144\n",
      "Epoch = 168 Step =    1510 loss = 0.143\n",
      "Epoch = 169 Step =    1520 loss = 0.142\n",
      "Epoch = 170 Step =    1530 loss = 0.142\n",
      "Epoch = 172 Step =    1540 loss = 0.141\n",
      "Epoch = 173 Step =    1550 loss = 0.140\n",
      "Epoch = 174 Step =    1560 loss = 0.139\n",
      "Epoch = 175 Step =    1570 loss = 0.138\n",
      "Epoch = 176 Step =    1580 loss = 0.138\n",
      "Epoch = 177 Step =    1590 loss = 0.137\n",
      "Epoch = 178 Step =    1600 loss = 0.136\n",
      "Epoch = 179 Step =    1610 loss = 0.135\n",
      "Epoch = 180 Step =    1620 loss = 0.135\n",
      "Epoch = 182 Step =    1630 loss = 0.134\n",
      "Epoch = 183 Step =    1640 loss = 0.133\n",
      "Epoch = 184 Step =    1650 loss = 0.132\n",
      "Epoch = 185 Step =    1660 loss = 0.132\n",
      "Epoch = 186 Step =    1670 loss = 0.131\n",
      "Epoch = 187 Step =    1680 loss = 0.130\n",
      "Epoch = 188 Step =    1690 loss = 0.130\n",
      "Epoch = 189 Step =    1700 loss = 0.129\n",
      "Epoch = 190 Step =    1710 loss = 0.128\n",
      "Epoch = 192 Step =    1720 loss = 0.128\n",
      "Epoch = 193 Step =    1730 loss = 0.127\n",
      "Epoch = 194 Step =    1740 loss = 0.127\n",
      "Epoch = 195 Step =    1750 loss = 0.126\n",
      "Epoch = 196 Step =    1760 loss = 0.125\n",
      "Epoch = 197 Step =    1770 loss = 0.125\n",
      "Epoch = 198 Step =    1780 loss = 0.124\n",
      "Epoch = 199 Step =    1790 loss = 0.124\n",
      "Epoch = 200 Step =    1800 loss = 0.123\n",
      "Epoch = 202 Step =    1810 loss = 0.122\n",
      "Epoch = 203 Step =    1820 loss = 0.122\n",
      "Epoch = 204 Step =    1830 loss = 0.121\n",
      "Epoch = 205 Step =    1840 loss = 0.120\n",
      "Epoch = 206 Step =    1850 loss = 0.120\n",
      "Epoch = 207 Step =    1860 loss = 0.119\n",
      "Epoch = 208 Step =    1870 loss = 0.119\n",
      "Epoch = 209 Step =    1880 loss = 0.118\n",
      "Epoch = 210 Step =    1890 loss = 0.118\n",
      "Epoch = 212 Step =    1900 loss = 0.117\n",
      "Epoch = 213 Step =    1910 loss = 0.117\n",
      "Epoch = 214 Step =    1920 loss = 0.116\n",
      "Epoch = 215 Step =    1930 loss = 0.115\n",
      "Epoch = 216 Step =    1940 loss = 0.115\n",
      "Epoch = 217 Step =    1950 loss = 0.114\n",
      "Epoch = 218 Step =    1960 loss = 0.114\n",
      "Epoch = 219 Step =    1970 loss = 0.114\n",
      "Epoch = 220 Step =    1980 loss = 0.113\n",
      "Epoch = 222 Step =    1990 loss = 0.113\n",
      "Epoch = 223 Step =    2000 loss = 0.112\n",
      "Epoch = 224 Step =    2010 loss = 0.112\n",
      "Epoch = 225 Step =    2020 loss = 0.111\n",
      "Epoch = 226 Step =    2030 loss = 0.111\n",
      "Epoch = 227 Step =    2040 loss = 0.110\n",
      "Epoch = 228 Step =    2050 loss = 0.110\n",
      "Epoch = 229 Step =    2060 loss = 0.109\n",
      "Epoch = 230 Step =    2070 loss = 0.109\n",
      "Epoch = 232 Step =    2080 loss = 0.108\n",
      "Epoch = 233 Step =    2090 loss = 0.108\n",
      "Epoch = 234 Step =    2100 loss = 0.107\n",
      "Epoch = 235 Step =    2110 loss = 0.107\n",
      "INFO:tensorflow:model/global_step/sec: 17.6581\n",
      "Epoch = 236 Step =    2120 loss = 0.107\n",
      "Epoch = 237 Step =    2130 loss = 0.106\n",
      "Epoch = 238 Step =    2140 loss = 0.106\n",
      "Epoch = 239 Step =    2150 loss = 0.105\n",
      "Epoch = 240 Step =    2160 loss = 0.105\n",
      "Epoch = 242 Step =    2170 loss = 0.104\n",
      "Epoch = 243 Step =    2180 loss = 0.104\n",
      "Epoch = 244 Step =    2190 loss = 0.103\n",
      "Epoch = 245 Step =    2200 loss = 0.103\n",
      "Epoch = 246 Step =    2210 loss = 0.103\n",
      "Epoch = 247 Step =    2220 loss = 0.102\n",
      "Epoch = 248 Step =    2230 loss = 0.102\n",
      "Epoch = 249 Step =    2240 loss = 0.101\n",
      "Epoch = 250 Step =    2250 loss = 0.101\n",
      "Epoch = 252 Step =    2260 loss = 0.101\n",
      "Epoch = 253 Step =    2270 loss = 0.100\n",
      "Epoch = 254 Step =    2280 loss = 0.100\n",
      "Epoch = 255 Step =    2290 loss = 0.099\n",
      "Epoch = 256 Step =    2300 loss = 0.099\n",
      "Epoch = 257 Step =    2310 loss = 0.099\n",
      "Epoch = 258 Step =    2320 loss = 0.098\n",
      "Epoch = 259 Step =    2330 loss = 0.098\n",
      "Epoch = 260 Step =    2340 loss = 0.097\n",
      "Epoch = 262 Step =    2350 loss = 0.097\n",
      "Epoch = 263 Step =    2360 loss = 0.097\n",
      "Epoch = 264 Step =    2370 loss = 0.096\n",
      "Epoch = 265 Step =    2380 loss = 0.096\n",
      "Epoch = 266 Step =    2390 loss = 0.096\n",
      "Epoch = 267 Step =    2400 loss = 0.095\n",
      "Epoch = 268 Step =    2410 loss = 0.095\n",
      "Epoch = 269 Step =    2420 loss = 0.094\n",
      "Epoch = 270 Step =    2430 loss = 0.094\n",
      "Epoch = 272 Step =    2440 loss = 0.094\n",
      "Epoch = 273 Step =    2450 loss = 0.093\n",
      "Epoch = 274 Step =    2460 loss = 0.093\n",
      "Epoch = 275 Step =    2470 loss = 0.093\n",
      "Epoch = 276 Step =    2480 loss = 0.092\n",
      "Epoch = 277 Step =    2490 loss = 0.092\n",
      "Epoch = 278 Step =    2500 loss = 0.092\n",
      "Epoch = 279 Step =    2510 loss = 0.091\n",
      "Epoch = 280 Step =    2520 loss = 0.091\n",
      "Epoch = 282 Step =    2530 loss = 0.091\n",
      "Epoch = 283 Step =    2540 loss = 0.090\n",
      "Epoch = 284 Step =    2550 loss = 0.090\n",
      "Epoch = 285 Step =    2560 loss = 0.090\n",
      "Epoch = 286 Step =    2570 loss = 0.089\n",
      "Epoch = 287 Step =    2580 loss = 0.089\n",
      "Epoch = 288 Step =    2590 loss = 0.089\n",
      "Epoch = 289 Step =    2600 loss = 0.088\n",
      "Epoch = 290 Step =    2610 loss = 0.088\n",
      "Epoch = 292 Step =    2620 loss = 0.088\n",
      "Epoch = 293 Step =    2630 loss = 0.087\n",
      "Epoch = 294 Step =    2640 loss = 0.087\n",
      "Epoch = 295 Step =    2650 loss = 0.087\n",
      "Epoch = 296 Step =    2660 loss = 0.087\n",
      "Epoch = 297 Step =    2670 loss = 0.086\n",
      "Epoch = 298 Step =    2680 loss = 0.086\n",
      "Epoch = 299 Step =    2690 loss = 0.086\n",
      "Epoch = 300 Step =    2700 loss = 0.086\n",
      "Epoch = 302 Step =    2710 loss = 0.085\n",
      "Epoch = 303 Step =    2720 loss = 0.085\n",
      "Epoch = 304 Step =    2730 loss = 0.085\n",
      "Epoch = 305 Step =    2740 loss = 0.085\n",
      "Epoch = 306 Step =    2750 loss = 0.084\n",
      "Epoch = 307 Step =    2760 loss = 0.084\n",
      "Epoch = 308 Step =    2770 loss = 0.084\n",
      "Epoch = 309 Step =    2780 loss = 0.083\n",
      "Epoch = 310 Step =    2790 loss = 0.083\n",
      "Epoch = 312 Step =    2800 loss = 0.083\n",
      "Epoch = 313 Step =    2810 loss = 0.083\n",
      "Epoch = 314 Step =    2820 loss = 0.082\n",
      "Epoch = 315 Step =    2830 loss = 0.082\n",
      "Epoch = 316 Step =    2840 loss = 0.082\n",
      "Epoch = 317 Step =    2850 loss = 0.082\n",
      "Epoch = 318 Step =    2860 loss = 0.081\n",
      "Epoch = 319 Step =    2870 loss = 0.081\n",
      "Epoch = 320 Step =    2880 loss = 0.081\n",
      "Epoch = 322 Step =    2890 loss = 0.080\n",
      "Epoch = 323 Step =    2900 loss = 0.080\n",
      "Epoch = 324 Step =    2910 loss = 0.080\n",
      "Epoch = 325 Step =    2920 loss = 0.080\n",
      "Epoch = 326 Step =    2930 loss = 0.079\n",
      "Epoch = 327 Step =    2940 loss = 0.079\n",
      "Epoch = 328 Step =    2950 loss = 0.079\n",
      "Epoch = 329 Step =    2960 loss = 0.079\n",
      "Epoch = 330 Step =    2970 loss = 0.079\n",
      "Epoch = 332 Step =    2980 loss = 0.078\n",
      "Epoch = 333 Step =    2990 loss = 0.078\n",
      "Epoch = 334 Step =    3000 loss = 0.078\n",
      "Epoch = 335 Step =    3010 loss = 0.078\n",
      "Epoch = 336 Step =    3020 loss = 0.077\n",
      "Epoch = 337 Step =    3030 loss = 0.077\n",
      "Epoch = 338 Step =    3040 loss = 0.077\n",
      "Epoch = 339 Step =    3050 loss = 0.077\n",
      "Epoch = 340 Step =    3060 loss = 0.077\n",
      "Epoch = 342 Step =    3070 loss = 0.077\n",
      "Epoch = 343 Step =    3080 loss = 0.076\n",
      "Epoch = 344 Step =    3090 loss = 0.076\n",
      "Epoch = 345 Step =    3100 loss = 0.076\n",
      "Epoch = 346 Step =    3110 loss = 0.076\n",
      "Epoch = 347 Step =    3120 loss = 0.075\n",
      "Epoch = 348 Step =    3130 loss = 0.075\n",
      "Epoch = 349 Step =    3140 loss = 0.075\n",
      "Epoch = 350 Step =    3150 loss = 0.075\n",
      "Epoch = 352 Step =    3160 loss = 0.075\n",
      "Epoch = 353 Step =    3170 loss = 0.074\n",
      "Epoch = 354 Step =    3180 loss = 0.074\n",
      "Epoch = 355 Step =    3190 loss = 0.074\n",
      "Epoch = 356 Step =    3200 loss = 0.074\n",
      "Epoch = 357 Step =    3210 loss = 0.073\n",
      "Epoch = 358 Step =    3220 loss = 0.073\n",
      "Epoch = 359 Step =    3230 loss = 0.073\n",
      "Epoch = 360 Step =    3240 loss = 0.073\n",
      "Epoch = 362 Step =    3250 loss = 0.073\n",
      "Epoch = 363 Step =    3260 loss = 0.073\n",
      "Epoch = 364 Step =    3270 loss = 0.072\n",
      "Epoch = 365 Step =    3280 loss = 0.072\n",
      "Epoch = 366 Step =    3290 loss = 0.072\n",
      "Epoch = 367 Step =    3300 loss = 0.072\n",
      "Epoch = 368 Step =    3310 loss = 0.072\n",
      "Epoch = 369 Step =    3320 loss = 0.071\n",
      "Epoch = 370 Step =    3330 loss = 0.071\n",
      "Epoch = 372 Step =    3340 loss = 0.071\n",
      "Epoch = 373 Step =    3350 loss = 0.071\n",
      "Epoch = 374 Step =    3360 loss = 0.071\n",
      "Epoch = 375 Step =    3370 loss = 0.070\n",
      "Epoch = 376 Step =    3380 loss = 0.070\n",
      "Epoch = 377 Step =    3390 loss = 0.070\n",
      "Epoch = 378 Step =    3400 loss = 0.070\n",
      "Epoch = 379 Step =    3410 loss = 0.070\n",
      "Epoch = 380 Step =    3420 loss = 0.070\n",
      "Epoch = 382 Step =    3430 loss = 0.069\n",
      "Epoch = 383 Step =    3440 loss = 0.069\n",
      "Epoch = 384 Step =    3450 loss = 0.069\n",
      "Epoch = 385 Step =    3460 loss = 0.069\n",
      "Epoch = 386 Step =    3470 loss = 0.069\n",
      "Epoch = 387 Step =    3480 loss = 0.069\n",
      "Epoch = 388 Step =    3490 loss = 0.068\n",
      "Epoch = 389 Step =    3500 loss = 0.068\n",
      "Epoch = 390 Step =    3510 loss = 0.068\n",
      "Epoch = 392 Step =    3520 loss = 0.068\n",
      "Epoch = 393 Step =    3530 loss = 0.068\n",
      "Epoch = 394 Step =    3540 loss = 0.067\n",
      "Epoch = 395 Step =    3550 loss = 0.067\n",
      "Epoch = 396 Step =    3560 loss = 0.067\n",
      "Epoch = 397 Step =    3570 loss = 0.067\n",
      "Epoch = 398 Step =    3580 loss = 0.067\n",
      "Epoch = 399 Step =    3590 loss = 0.067\n",
      "Epoch = 400 Step =    3600 loss = 0.066\n",
      "Epoch = 402 Step =    3610 loss = 0.066\n",
      "Epoch = 403 Step =    3620 loss = 0.066\n",
      "Epoch = 404 Step =    3630 loss = 0.066\n",
      "Epoch = 405 Step =    3640 loss = 0.066\n",
      "Epoch = 406 Step =    3650 loss = 0.066\n",
      "Epoch = 407 Step =    3660 loss = 0.066\n",
      "Epoch = 408 Step =    3670 loss = 0.065\n",
      "Epoch = 409 Step =    3680 loss = 0.065\n",
      "Epoch = 410 Step =    3690 loss = 0.065\n",
      "Epoch = 412 Step =    3700 loss = 0.065\n",
      "Epoch = 413 Step =    3710 loss = 0.065\n",
      "Epoch = 414 Step =    3720 loss = 0.065\n",
      "Epoch = 415 Step =    3730 loss = 0.064\n",
      "Epoch = 416 Step =    3740 loss = 0.064\n",
      "Epoch = 417 Step =    3750 loss = 0.064\n",
      "Epoch = 418 Step =    3760 loss = 0.064\n",
      "Epoch = 419 Step =    3770 loss = 0.064\n",
      "Epoch = 420 Step =    3780 loss = 0.064\n",
      "Epoch = 422 Step =    3790 loss = 0.064\n",
      "Epoch = 423 Step =    3800 loss = 0.063\n",
      "Epoch = 424 Step =    3810 loss = 0.063\n",
      "Epoch = 425 Step =    3820 loss = 0.063\n",
      "Epoch = 426 Step =    3830 loss = 0.063\n",
      "Epoch = 427 Step =    3840 loss = 0.063\n",
      "Epoch = 428 Step =    3850 loss = 0.063\n",
      "Epoch = 429 Step =    3860 loss = 0.062\n",
      "Epoch = 430 Step =    3870 loss = 0.062\n",
      "Epoch = 432 Step =    3880 loss = 0.062\n",
      "Epoch = 433 Step =    3890 loss = 0.062\n",
      "Epoch = 434 Step =    3900 loss = 0.062\n",
      "Epoch = 435 Step =    3910 loss = 0.062\n",
      "Epoch = 436 Step =    3920 loss = 0.062\n",
      "Epoch = 437 Step =    3930 loss = 0.061\n",
      "Epoch = 438 Step =    3940 loss = 0.061\n",
      "Epoch = 439 Step =    3950 loss = 0.061\n",
      "Epoch = 440 Step =    3960 loss = 0.061\n",
      "Epoch = 442 Step =    3970 loss = 0.061\n",
      "Epoch = 443 Step =    3980 loss = 0.061\n",
      "Epoch = 444 Step =    3990 loss = 0.061\n",
      "Epoch = 445 Step =    4000 loss = 0.060\n",
      "Epoch = 446 Step =    4010 loss = 0.060\n",
      "Epoch = 447 Step =    4020 loss = 0.060\n",
      "Epoch = 448 Step =    4030 loss = 0.060\n",
      "Epoch = 449 Step =    4040 loss = 0.060\n",
      "Epoch = 450 Step =    4050 loss = 0.060\n",
      "Epoch = 452 Step =    4060 loss = 0.060\n",
      "Epoch = 453 Step =    4070 loss = 0.059\n",
      "Epoch = 454 Step =    4080 loss = 0.059\n",
      "Epoch = 455 Step =    4090 loss = 0.059\n",
      "Epoch = 456 Step =    4100 loss = 0.059\n",
      "Epoch = 457 Step =    4110 loss = 0.059\n",
      "Epoch = 458 Step =    4120 loss = 0.059\n",
      "INFO:tensorflow:model/global_step/sec: 16.7502\n",
      "Epoch = 459 Step =    4130 loss = 0.059\n",
      "Epoch = 460 Step =    4140 loss = 0.059\n",
      "Epoch = 462 Step =    4150 loss = 0.058\n",
      "Epoch = 463 Step =    4160 loss = 0.058\n",
      "Epoch = 464 Step =    4170 loss = 0.058\n",
      "Epoch = 465 Step =    4180 loss = 0.058\n",
      "Epoch = 466 Step =    4190 loss = 0.058\n",
      "Epoch = 467 Step =    4200 loss = 0.058\n",
      "Epoch = 468 Step =    4210 loss = 0.058\n",
      "Epoch = 469 Step =    4220 loss = 0.058\n",
      "Epoch = 470 Step =    4230 loss = 0.057\n",
      "Epoch = 472 Step =    4240 loss = 0.057\n",
      "Epoch = 473 Step =    4250 loss = 0.057\n",
      "Epoch = 474 Step =    4260 loss = 0.057\n",
      "Epoch = 475 Step =    4270 loss = 0.057\n",
      "Epoch = 476 Step =    4280 loss = 0.057\n",
      "Epoch = 477 Step =    4290 loss = 0.057\n",
      "Epoch = 478 Step =    4300 loss = 0.057\n",
      "Epoch = 479 Step =    4310 loss = 0.057\n",
      "Epoch = 480 Step =    4320 loss = 0.057\n",
      "Epoch = 482 Step =    4330 loss = 0.056\n",
      "Epoch = 483 Step =    4340 loss = 0.056\n",
      "Epoch = 484 Step =    4350 loss = 0.056\n",
      "Epoch = 485 Step =    4360 loss = 0.056\n",
      "Epoch = 486 Step =    4370 loss = 0.056\n",
      "Epoch = 487 Step =    4380 loss = 0.056\n",
      "Epoch = 488 Step =    4390 loss = 0.056\n",
      "Epoch = 489 Step =    4400 loss = 0.056\n",
      "Epoch = 490 Step =    4410 loss = 0.055\n",
      "Epoch = 492 Step =    4420 loss = 0.055\n",
      "Epoch = 493 Step =    4430 loss = 0.055\n",
      "Epoch = 494 Step =    4440 loss = 0.055\n",
      "Epoch = 495 Step =    4450 loss = 0.055\n",
      "Epoch = 496 Step =    4460 loss = 0.055\n",
      "Epoch = 497 Step =    4470 loss = 0.055\n",
      "Epoch = 498 Step =    4480 loss = 0.055\n",
      "Epoch = 499 Step =    4490 loss = 0.055\n",
      "Epoch = 500 Step =    4500 loss = 0.054\n",
      "Epoch = 502 Step =    4510 loss = 0.054\n",
      "Epoch = 503 Step =    4520 loss = 0.054\n",
      "Epoch = 504 Step =    4530 loss = 0.054\n",
      "Epoch = 505 Step =    4540 loss = 0.054\n",
      "Epoch = 506 Step =    4550 loss = 0.054\n",
      "Epoch = 507 Step =    4560 loss = 0.054\n",
      "Epoch = 508 Step =    4570 loss = 0.054\n",
      "Epoch = 509 Step =    4580 loss = 0.054\n",
      "Epoch = 510 Step =    4590 loss = 0.053\n",
      "Epoch = 512 Step =    4600 loss = 0.053\n",
      "Epoch = 513 Step =    4610 loss = 0.053\n",
      "Epoch = 514 Step =    4620 loss = 0.053\n",
      "Epoch = 515 Step =    4630 loss = 0.053\n",
      "Epoch = 516 Step =    4640 loss = 0.053\n",
      "Epoch = 517 Step =    4650 loss = 0.053\n",
      "Epoch = 518 Step =    4660 loss = 0.053\n",
      "Epoch = 519 Step =    4670 loss = 0.053\n",
      "Epoch = 520 Step =    4680 loss = 0.053\n",
      "Epoch = 522 Step =    4690 loss = 0.053\n",
      "Epoch = 523 Step =    4700 loss = 0.053\n",
      "Epoch = 524 Step =    4710 loss = 0.052\n",
      "Epoch = 525 Step =    4720 loss = 0.052\n",
      "Epoch = 526 Step =    4730 loss = 0.052\n",
      "Epoch = 527 Step =    4740 loss = 0.052\n",
      "Epoch = 528 Step =    4750 loss = 0.052\n",
      "Epoch = 529 Step =    4760 loss = 0.052\n",
      "Epoch = 530 Step =    4770 loss = 0.052\n",
      "Epoch = 532 Step =    4780 loss = 0.052\n",
      "Epoch = 533 Step =    4790 loss = 0.052\n",
      "Epoch = 534 Step =    4800 loss = 0.051\n",
      "Epoch = 535 Step =    4810 loss = 0.051\n",
      "Epoch = 536 Step =    4820 loss = 0.051\n",
      "Epoch = 537 Step =    4830 loss = 0.051\n",
      "Epoch = 538 Step =    4840 loss = 0.051\n",
      "Epoch = 539 Step =    4850 loss = 0.051\n",
      "Epoch = 540 Step =    4860 loss = 0.051\n",
      "Epoch = 542 Step =    4870 loss = 0.051\n",
      "Epoch = 543 Step =    4880 loss = 0.051\n",
      "Epoch = 544 Step =    4890 loss = 0.051\n",
      "Epoch = 545 Step =    4900 loss = 0.051\n",
      "Epoch = 546 Step =    4910 loss = 0.050\n",
      "Epoch = 547 Step =    4920 loss = 0.050\n",
      "Epoch = 548 Step =    4930 loss = 0.050\n",
      "Epoch = 549 Step =    4940 loss = 0.050\n",
      "Epoch = 550 Step =    4950 loss = 0.050\n",
      "Epoch = 552 Step =    4960 loss = 0.050\n",
      "Epoch = 553 Step =    4970 loss = 0.050\n",
      "Epoch = 554 Step =    4980 loss = 0.050\n",
      "Epoch = 555 Step =    4990 loss = 0.050\n",
      "Epoch = 556 Step =    5000 loss = 0.050\n",
      "Epoch = 557 Step =    5010 loss = 0.049\n",
      "Epoch = 558 Step =    5020 loss = 0.049\n",
      "Epoch = 559 Step =    5030 loss = 0.049\n",
      "Epoch = 560 Step =    5040 loss = 0.049\n",
      "Epoch = 562 Step =    5050 loss = 0.049\n",
      "Epoch = 563 Step =    5060 loss = 0.049\n",
      "Epoch = 564 Step =    5070 loss = 0.049\n",
      "Epoch = 565 Step =    5080 loss = 0.049\n",
      "Epoch = 566 Step =    5090 loss = 0.049\n",
      "Epoch = 567 Step =    5100 loss = 0.049\n",
      "Epoch = 568 Step =    5110 loss = 0.049\n",
      "Epoch = 569 Step =    5120 loss = 0.048\n",
      "Epoch = 570 Step =    5130 loss = 0.048\n",
      "Epoch = 572 Step =    5140 loss = 0.048\n",
      "Epoch = 573 Step =    5150 loss = 0.048\n",
      "Epoch = 574 Step =    5160 loss = 0.048\n",
      "Epoch = 575 Step =    5170 loss = 0.048\n",
      "Epoch = 576 Step =    5180 loss = 0.048\n",
      "Epoch = 577 Step =    5190 loss = 0.048\n",
      "Epoch = 578 Step =    5200 loss = 0.048\n",
      "Epoch = 579 Step =    5210 loss = 0.048\n",
      "Epoch = 580 Step =    5220 loss = 0.048\n",
      "Epoch = 582 Step =    5230 loss = 0.048\n",
      "Epoch = 583 Step =    5240 loss = 0.048\n",
      "Epoch = 584 Step =    5250 loss = 0.047\n",
      "Epoch = 585 Step =    5260 loss = 0.047\n",
      "Epoch = 586 Step =    5270 loss = 0.047\n",
      "Epoch = 587 Step =    5280 loss = 0.047\n",
      "Epoch = 588 Step =    5290 loss = 0.047\n",
      "Epoch = 589 Step =    5300 loss = 0.047\n",
      "Epoch = 590 Step =    5310 loss = 0.047\n",
      "Epoch = 592 Step =    5320 loss = 0.047\n",
      "Epoch = 593 Step =    5330 loss = 0.047\n",
      "Epoch = 594 Step =    5340 loss = 0.047\n",
      "Epoch = 595 Step =    5350 loss = 0.047\n",
      "Epoch = 596 Step =    5360 loss = 0.047\n",
      "Epoch = 597 Step =    5370 loss = 0.046\n",
      "Epoch = 598 Step =    5380 loss = 0.046\n",
      "Epoch = 599 Step =    5390 loss = 0.046\n",
      "Epoch = 600 Step =    5400 loss = 0.046\n",
      "Epoch = 602 Step =    5410 loss = 0.046\n",
      "Epoch = 603 Step =    5420 loss = 0.046\n",
      "Epoch = 604 Step =    5430 loss = 0.046\n",
      "Epoch = 605 Step =    5440 loss = 0.046\n",
      "Epoch = 606 Step =    5450 loss = 0.046\n",
      "Epoch = 607 Step =    5460 loss = 0.046\n",
      "Epoch = 608 Step =    5470 loss = 0.046\n",
      "Epoch = 609 Step =    5480 loss = 0.046\n",
      "Epoch = 610 Step =    5490 loss = 0.046\n",
      "Epoch = 612 Step =    5500 loss = 0.045\n",
      "Epoch = 613 Step =    5510 loss = 0.045\n",
      "Epoch = 614 Step =    5520 loss = 0.045\n",
      "Epoch = 615 Step =    5530 loss = 0.045\n",
      "Epoch = 616 Step =    5540 loss = 0.045\n",
      "Epoch = 617 Step =    5550 loss = 0.045\n",
      "Epoch = 618 Step =    5560 loss = 0.045\n",
      "Epoch = 619 Step =    5570 loss = 0.045\n",
      "Epoch = 620 Step =    5580 loss = 0.045\n",
      "Epoch = 622 Step =    5590 loss = 0.045\n",
      "Epoch = 623 Step =    5600 loss = 0.045\n",
      "Epoch = 624 Step =    5610 loss = 0.045\n",
      "Epoch = 625 Step =    5620 loss = 0.045\n",
      "Epoch = 626 Step =    5630 loss = 0.045\n",
      "Epoch = 627 Step =    5640 loss = 0.044\n",
      "Epoch = 628 Step =    5650 loss = 0.044\n",
      "Epoch = 629 Step =    5660 loss = 0.044\n",
      "Epoch = 630 Step =    5670 loss = 0.044\n",
      "Epoch = 632 Step =    5680 loss = 0.044\n",
      "Epoch = 633 Step =    5690 loss = 0.044\n",
      "Epoch = 634 Step =    5700 loss = 0.044\n",
      "Epoch = 635 Step =    5710 loss = 0.044\n",
      "Epoch = 636 Step =    5720 loss = 0.044\n",
      "Epoch = 637 Step =    5730 loss = 0.044\n",
      "Epoch = 638 Step =    5740 loss = 0.044\n",
      "Epoch = 639 Step =    5750 loss = 0.044\n",
      "Epoch = 640 Step =    5760 loss = 0.044\n",
      "Epoch = 642 Step =    5770 loss = 0.044\n",
      "Epoch = 643 Step =    5780 loss = 0.043\n",
      "Epoch = 644 Step =    5790 loss = 0.043\n",
      "Epoch = 645 Step =    5800 loss = 0.043\n",
      "Epoch = 646 Step =    5810 loss = 0.043\n",
      "Epoch = 647 Step =    5820 loss = 0.043\n",
      "Epoch = 648 Step =    5830 loss = 0.043\n",
      "Epoch = 649 Step =    5840 loss = 0.043\n",
      "Epoch = 650 Step =    5850 loss = 0.043\n",
      "Epoch = 652 Step =    5860 loss = 0.043\n",
      "Epoch = 653 Step =    5870 loss = 0.043\n",
      "Epoch = 654 Step =    5880 loss = 0.043\n",
      "Epoch = 655 Step =    5890 loss = 0.043\n",
      "Epoch = 656 Step =    5900 loss = 0.043\n",
      "Epoch = 657 Step =    5910 loss = 0.043\n",
      "Epoch = 658 Step =    5920 loss = 0.043\n",
      "Epoch = 659 Step =    5930 loss = 0.042\n",
      "Epoch = 660 Step =    5940 loss = 0.042\n",
      "Epoch = 662 Step =    5950 loss = 0.042\n",
      "Epoch = 663 Step =    5960 loss = 0.042\n",
      "Epoch = 664 Step =    5970 loss = 0.042\n",
      "Epoch = 665 Step =    5980 loss = 0.042\n",
      "Epoch = 666 Step =    5990 loss = 0.042\n",
      "Epoch = 667 Step =    6000 loss = 0.042\n",
      "Epoch = 668 Step =    6010 loss = 0.042\n",
      "Epoch = 669 Step =    6020 loss = 0.042\n",
      "Epoch = 670 Step =    6030 loss = 0.042\n",
      "Epoch = 672 Step =    6040 loss = 0.042\n",
      "Epoch = 673 Step =    6050 loss = 0.042\n",
      "Epoch = 674 Step =    6060 loss = 0.042\n",
      "Epoch = 675 Step =    6070 loss = 0.042\n",
      "Epoch = 676 Step =    6080 loss = 0.042\n",
      "Epoch = 677 Step =    6090 loss = 0.041\n",
      "Epoch = 678 Step =    6100 loss = 0.041\n",
      "Epoch = 679 Step =    6110 loss = 0.041\n",
      "Epoch = 680 Step =    6120 loss = 0.041\n",
      "Epoch = 682 Step =    6130 loss = 0.041\n",
      "Epoch = 683 Step =    6140 loss = 0.041\n",
      "Epoch = 684 Step =    6150 loss = 0.041\n",
      "Epoch = 685 Step =    6160 loss = 0.041\n",
      "Epoch = 686 Step =    6170 loss = 0.041\n",
      "Epoch = 687 Step =    6180 loss = 0.041\n",
      "Epoch = 688 Step =    6190 loss = 0.041\n",
      "Epoch = 689 Step =    6200 loss = 0.041\n",
      "Epoch = 690 Step =    6210 loss = 0.041\n",
      "INFO:tensorflow:model/global_step/sec: 17.4083\n",
      "Epoch = 692 Step =    6220 loss = 0.041\n",
      "Epoch = 693 Step =    6230 loss = 0.041\n",
      "Epoch = 694 Step =    6240 loss = 0.041\n",
      "Epoch = 695 Step =    6250 loss = 0.041\n",
      "Epoch = 696 Step =    6260 loss = 0.040\n",
      "Epoch = 697 Step =    6270 loss = 0.040\n",
      "Epoch = 698 Step =    6280 loss = 0.040\n",
      "Epoch = 699 Step =    6290 loss = 0.040\n",
      "Epoch = 700 Step =    6300 loss = 0.040\n",
      "Epoch = 702 Step =    6310 loss = 0.040\n",
      "Epoch = 703 Step =    6320 loss = 0.040\n",
      "Epoch = 704 Step =    6330 loss = 0.040\n",
      "Epoch = 705 Step =    6340 loss = 0.040\n",
      "Epoch = 706 Step =    6350 loss = 0.040\n",
      "Epoch = 707 Step =    6360 loss = 0.040\n",
      "Epoch = 708 Step =    6370 loss = 0.040\n",
      "Epoch = 709 Step =    6380 loss = 0.040\n",
      "Epoch = 710 Step =    6390 loss = 0.040\n",
      "Epoch = 712 Step =    6400 loss = 0.040\n",
      "Epoch = 713 Step =    6410 loss = 0.040\n",
      "Epoch = 714 Step =    6420 loss = 0.040\n",
      "Epoch = 715 Step =    6430 loss = 0.040\n",
      "Epoch = 716 Step =    6440 loss = 0.040\n",
      "Epoch = 717 Step =    6450 loss = 0.040\n",
      "Epoch = 718 Step =    6460 loss = 0.039\n",
      "Epoch = 719 Step =    6470 loss = 0.039\n",
      "Epoch = 720 Step =    6480 loss = 0.039\n",
      "Epoch = 722 Step =    6490 loss = 0.039\n",
      "Epoch = 723 Step =    6500 loss = 0.039\n",
      "Epoch = 724 Step =    6510 loss = 0.039\n",
      "Epoch = 725 Step =    6520 loss = 0.039\n",
      "Epoch = 726 Step =    6530 loss = 0.039\n",
      "Epoch = 727 Step =    6540 loss = 0.039\n",
      "Epoch = 728 Step =    6550 loss = 0.039\n",
      "Epoch = 729 Step =    6560 loss = 0.039\n",
      "Epoch = 730 Step =    6570 loss = 0.039\n",
      "Epoch = 732 Step =    6580 loss = 0.039\n",
      "Epoch = 733 Step =    6590 loss = 0.039\n",
      "Epoch = 734 Step =    6600 loss = 0.039\n",
      "Epoch = 735 Step =    6610 loss = 0.039\n",
      "Epoch = 736 Step =    6620 loss = 0.039\n",
      "Epoch = 737 Step =    6630 loss = 0.039\n",
      "Epoch = 738 Step =    6640 loss = 0.038\n",
      "Epoch = 739 Step =    6650 loss = 0.038\n",
      "Epoch = 740 Step =    6660 loss = 0.038\n",
      "Epoch = 742 Step =    6670 loss = 0.038\n",
      "Epoch = 743 Step =    6680 loss = 0.038\n",
      "Epoch = 744 Step =    6690 loss = 0.038\n",
      "Epoch = 745 Step =    6700 loss = 0.038\n",
      "Epoch = 746 Step =    6710 loss = 0.038\n",
      "Epoch = 747 Step =    6720 loss = 0.038\n",
      "Epoch = 748 Step =    6730 loss = 0.038\n",
      "Epoch = 749 Step =    6740 loss = 0.038\n",
      "Epoch = 750 Step =    6750 loss = 0.038\n",
      "Epoch = 752 Step =    6760 loss = 0.038\n",
      "Epoch = 753 Step =    6770 loss = 0.038\n",
      "Epoch = 754 Step =    6780 loss = 0.038\n",
      "Epoch = 755 Step =    6790 loss = 0.038\n",
      "Epoch = 756 Step =    6800 loss = 0.038\n",
      "Epoch = 757 Step =    6810 loss = 0.038\n",
      "Epoch = 758 Step =    6820 loss = 0.038\n",
      "Epoch = 759 Step =    6830 loss = 0.038\n",
      "Epoch = 760 Step =    6840 loss = 0.037\n",
      "Epoch = 762 Step =    6850 loss = 0.037\n",
      "Epoch = 763 Step =    6860 loss = 0.037\n",
      "Epoch = 764 Step =    6870 loss = 0.037\n",
      "Epoch = 765 Step =    6880 loss = 0.037\n",
      "Epoch = 766 Step =    6890 loss = 0.037\n",
      "Epoch = 767 Step =    6900 loss = 0.037\n",
      "Epoch = 768 Step =    6910 loss = 0.037\n",
      "Epoch = 769 Step =    6920 loss = 0.037\n",
      "Epoch = 770 Step =    6930 loss = 0.037\n",
      "Epoch = 772 Step =    6940 loss = 0.037\n",
      "Epoch = 773 Step =    6950 loss = 0.037\n",
      "Epoch = 774 Step =    6960 loss = 0.037\n",
      "Epoch = 775 Step =    6970 loss = 0.037\n",
      "Epoch = 776 Step =    6980 loss = 0.037\n",
      "Epoch = 777 Step =    6990 loss = 0.037\n",
      "Epoch = 778 Step =    7000 loss = 0.037\n",
      "Epoch = 779 Step =    7010 loss = 0.037\n",
      "Epoch = 780 Step =    7020 loss = 0.037\n",
      "Epoch = 782 Step =    7030 loss = 0.037\n",
      "Epoch = 783 Step =    7040 loss = 0.036\n",
      "Epoch = 784 Step =    7050 loss = 0.036\n",
      "Epoch = 785 Step =    7060 loss = 0.036\n",
      "Epoch = 786 Step =    7070 loss = 0.036\n",
      "Epoch = 787 Step =    7080 loss = 0.036\n",
      "Epoch = 788 Step =    7090 loss = 0.036\n",
      "Epoch = 789 Step =    7100 loss = 0.036\n",
      "Epoch = 790 Step =    7110 loss = 0.036\n",
      "Epoch = 792 Step =    7120 loss = 0.036\n",
      "Epoch = 793 Step =    7130 loss = 0.036\n",
      "Epoch = 794 Step =    7140 loss = 0.036\n",
      "Epoch = 795 Step =    7150 loss = 0.036\n",
      "Epoch = 796 Step =    7160 loss = 0.036\n",
      "Epoch = 797 Step =    7170 loss = 0.036\n",
      "Epoch = 798 Step =    7180 loss = 0.036\n",
      "Epoch = 799 Step =    7190 loss = 0.036\n",
      "Epoch = 800 Step =    7200 loss = 0.036\n",
      "Epoch = 802 Step =    7210 loss = 0.036\n",
      "Epoch = 803 Step =    7220 loss = 0.036\n",
      "Epoch = 804 Step =    7230 loss = 0.036\n",
      "Epoch = 805 Step =    7240 loss = 0.036\n",
      "Epoch = 806 Step =    7250 loss = 0.036\n",
      "Epoch = 807 Step =    7260 loss = 0.036\n",
      "Epoch = 808 Step =    7270 loss = 0.035\n",
      "Epoch = 809 Step =    7280 loss = 0.035\n",
      "Epoch = 810 Step =    7290 loss = 0.035\n",
      "Epoch = 812 Step =    7300 loss = 0.035\n",
      "Epoch = 813 Step =    7310 loss = 0.035\n",
      "Epoch = 814 Step =    7320 loss = 0.035\n",
      "Epoch = 815 Step =    7330 loss = 0.035\n",
      "Epoch = 816 Step =    7340 loss = 0.035\n",
      "Epoch = 817 Step =    7350 loss = 0.035\n",
      "Epoch = 818 Step =    7360 loss = 0.035\n",
      "Epoch = 819 Step =    7370 loss = 0.035\n",
      "Epoch = 820 Step =    7380 loss = 0.035\n",
      "Epoch = 822 Step =    7390 loss = 0.035\n",
      "Epoch = 823 Step =    7400 loss = 0.035\n",
      "Epoch = 824 Step =    7410 loss = 0.035\n",
      "Epoch = 825 Step =    7420 loss = 0.035\n",
      "Epoch = 826 Step =    7430 loss = 0.035\n",
      "Epoch = 827 Step =    7440 loss = 0.035\n",
      "Epoch = 828 Step =    7450 loss = 0.035\n",
      "Epoch = 829 Step =    7460 loss = 0.035\n",
      "Epoch = 830 Step =    7470 loss = 0.035\n",
      "Epoch = 832 Step =    7480 loss = 0.035\n",
      "Epoch = 833 Step =    7490 loss = 0.035\n",
      "Epoch = 834 Step =    7500 loss = 0.035\n",
      "Epoch = 835 Step =    7510 loss = 0.035\n",
      "Epoch = 836 Step =    7520 loss = 0.035\n",
      "Epoch = 837 Step =    7530 loss = 0.035\n",
      "Epoch = 838 Step =    7540 loss = 0.034\n",
      "Epoch = 839 Step =    7550 loss = 0.034\n",
      "Epoch = 840 Step =    7560 loss = 0.034\n",
      "Epoch = 842 Step =    7570 loss = 0.034\n",
      "Epoch = 843 Step =    7580 loss = 0.034\n",
      "Epoch = 844 Step =    7590 loss = 0.034\n",
      "Epoch = 845 Step =    7600 loss = 0.034\n",
      "Epoch = 846 Step =    7610 loss = 0.034\n",
      "Epoch = 847 Step =    7620 loss = 0.034\n",
      "Epoch = 848 Step =    7630 loss = 0.034\n",
      "Epoch = 849 Step =    7640 loss = 0.034\n",
      "Epoch = 850 Step =    7650 loss = 0.034\n",
      "Epoch = 852 Step =    7660 loss = 0.034\n",
      "Epoch = 853 Step =    7670 loss = 0.034\n",
      "Epoch = 854 Step =    7680 loss = 0.034\n",
      "Epoch = 855 Step =    7690 loss = 0.034\n",
      "Epoch = 856 Step =    7700 loss = 0.034\n",
      "Epoch = 857 Step =    7710 loss = 0.034\n",
      "Epoch = 858 Step =    7720 loss = 0.034\n",
      "Epoch = 859 Step =    7730 loss = 0.034\n",
      "Epoch = 860 Step =    7740 loss = 0.034\n",
      "Epoch = 862 Step =    7750 loss = 0.034\n",
      "Epoch = 863 Step =    7760 loss = 0.034\n",
      "Epoch = 864 Step =    7770 loss = 0.033\n",
      "Epoch = 865 Step =    7780 loss = 0.033\n",
      "Epoch = 866 Step =    7790 loss = 0.033\n",
      "Epoch = 867 Step =    7800 loss = 0.033\n",
      "Epoch = 868 Step =    7810 loss = 0.033\n",
      "Epoch = 869 Step =    7820 loss = 0.033\n",
      "Epoch = 870 Step =    7830 loss = 0.033\n",
      "Epoch = 872 Step =    7840 loss = 0.033\n",
      "Epoch = 873 Step =    7850 loss = 0.033\n",
      "Epoch = 874 Step =    7860 loss = 0.033\n",
      "Epoch = 875 Step =    7870 loss = 0.033\n",
      "Epoch = 876 Step =    7880 loss = 0.033\n",
      "Epoch = 877 Step =    7890 loss = 0.033\n",
      "Epoch = 878 Step =    7900 loss = 0.033\n",
      "Epoch = 879 Step =    7910 loss = 0.033\n",
      "Epoch = 880 Step =    7920 loss = 0.033\n",
      "Epoch = 882 Step =    7930 loss = 0.033\n",
      "Epoch = 883 Step =    7940 loss = 0.033\n",
      "Epoch = 884 Step =    7950 loss = 0.033\n",
      "Epoch = 885 Step =    7960 loss = 0.033\n",
      "Epoch = 886 Step =    7970 loss = 0.033\n",
      "Epoch = 887 Step =    7980 loss = 0.033\n",
      "Epoch = 888 Step =    7990 loss = 0.033\n",
      "Epoch = 889 Step =    8000 loss = 0.033\n",
      "Epoch = 890 Step =    8010 loss = 0.033\n",
      "Epoch = 892 Step =    8020 loss = 0.032\n",
      "Epoch = 893 Step =    8030 loss = 0.032\n",
      "Epoch = 894 Step =    8040 loss = 0.032\n",
      "Epoch = 895 Step =    8050 loss = 0.032\n",
      "Epoch = 896 Step =    8060 loss = 0.032\n",
      "Epoch = 897 Step =    8070 loss = 0.032\n",
      "Epoch = 898 Step =    8080 loss = 0.032\n",
      "Epoch = 899 Step =    8090 loss = 0.032\n",
      "Epoch = 900 Step =    8100 loss = 0.032\n",
      "Epoch = 902 Step =    8110 loss = 0.032\n",
      "Epoch = 903 Step =    8120 loss = 0.032\n",
      "Epoch = 904 Step =    8130 loss = 0.032\n",
      "Epoch = 905 Step =    8140 loss = 0.032\n",
      "Epoch = 906 Step =    8150 loss = 0.032\n",
      "Epoch = 907 Step =    8160 loss = 0.032\n",
      "Epoch = 908 Step =    8170 loss = 0.032\n",
      "Epoch = 909 Step =    8180 loss = 0.032\n",
      "Epoch = 910 Step =    8190 loss = 0.032\n",
      "Epoch = 912 Step =    8200 loss = 0.032\n",
      "Epoch = 913 Step =    8210 loss = 0.032\n",
      "Epoch = 914 Step =    8220 loss = 0.032\n",
      "Epoch = 915 Step =    8230 loss = 0.032\n",
      "Epoch = 916 Step =    8240 loss = 0.032\n",
      "Epoch = 917 Step =    8250 loss = 0.032\n",
      "Epoch = 918 Step =    8260 loss = 0.032\n",
      "Epoch = 919 Step =    8270 loss = 0.032\n",
      "Epoch = 920 Step =    8280 loss = 0.032\n",
      "Epoch = 922 Step =    8290 loss = 0.031\n",
      "Epoch = 923 Step =    8300 loss = 0.031\n",
      "Epoch = 924 Step =    8310 loss = 0.031\n",
      "INFO:tensorflow:model/global_step/sec: 17.4579\n",
      "Epoch = 925 Step =    8320 loss = 0.031\n",
      "Epoch = 926 Step =    8330 loss = 0.031\n",
      "Epoch = 927 Step =    8340 loss = 0.031\n",
      "Epoch = 928 Step =    8350 loss = 0.031\n",
      "Epoch = 929 Step =    8360 loss = 0.031\n",
      "Epoch = 930 Step =    8370 loss = 0.031\n",
      "Epoch = 932 Step =    8380 loss = 0.031\n",
      "Epoch = 933 Step =    8390 loss = 0.031\n",
      "Epoch = 934 Step =    8400 loss = 0.031\n",
      "Epoch = 935 Step =    8410 loss = 0.031\n",
      "Epoch = 936 Step =    8420 loss = 0.031\n",
      "Epoch = 937 Step =    8430 loss = 0.031\n",
      "Epoch = 938 Step =    8440 loss = 0.031\n",
      "Epoch = 939 Step =    8450 loss = 0.031\n",
      "Epoch = 940 Step =    8460 loss = 0.031\n",
      "Epoch = 942 Step =    8470 loss = 0.031\n",
      "Epoch = 943 Step =    8480 loss = 0.031\n",
      "Epoch = 944 Step =    8490 loss = 0.031\n",
      "Epoch = 945 Step =    8500 loss = 0.031\n",
      "Epoch = 946 Step =    8510 loss = 0.031\n",
      "Epoch = 947 Step =    8520 loss = 0.031\n",
      "Epoch = 948 Step =    8530 loss = 0.031\n",
      "Epoch = 949 Step =    8540 loss = 0.031\n",
      "Epoch = 950 Step =    8550 loss = 0.031\n",
      "Epoch = 952 Step =    8560 loss = 0.031\n",
      "Epoch = 953 Step =    8570 loss = 0.031\n",
      "Epoch = 954 Step =    8580 loss = 0.031\n",
      "Epoch = 955 Step =    8590 loss = 0.031\n",
      "Epoch = 956 Step =    8600 loss = 0.031\n",
      "Epoch = 957 Step =    8610 loss = 0.030\n",
      "Epoch = 958 Step =    8620 loss = 0.030\n",
      "Epoch = 959 Step =    8630 loss = 0.030\n",
      "Epoch = 960 Step =    8640 loss = 0.030\n",
      "Epoch = 962 Step =    8650 loss = 0.030\n",
      "Epoch = 963 Step =    8660 loss = 0.030\n",
      "Epoch = 964 Step =    8670 loss = 0.030\n",
      "Epoch = 965 Step =    8680 loss = 0.030\n",
      "Epoch = 966 Step =    8690 loss = 0.030\n",
      "Epoch = 967 Step =    8700 loss = 0.030\n",
      "Epoch = 968 Step =    8710 loss = 0.030\n",
      "Epoch = 969 Step =    8720 loss = 0.030\n",
      "Epoch = 970 Step =    8730 loss = 0.030\n",
      "Epoch = 972 Step =    8740 loss = 0.030\n",
      "Epoch = 973 Step =    8750 loss = 0.030\n",
      "Epoch = 974 Step =    8760 loss = 0.030\n",
      "Epoch = 975 Step =    8770 loss = 0.030\n",
      "Epoch = 976 Step =    8780 loss = 0.030\n",
      "Epoch = 977 Step =    8790 loss = 0.030\n",
      "Epoch = 978 Step =    8800 loss = 0.030\n",
      "Epoch = 979 Step =    8810 loss = 0.030\n",
      "Epoch = 980 Step =    8820 loss = 0.030\n",
      "Epoch = 982 Step =    8830 loss = 0.030\n",
      "Epoch = 983 Step =    8840 loss = 0.030\n",
      "Epoch = 984 Step =    8850 loss = 0.030\n",
      "Epoch = 985 Step =    8860 loss = 0.030\n",
      "Epoch = 986 Step =    8870 loss = 0.030\n",
      "Epoch = 987 Step =    8880 loss = 0.030\n",
      "Epoch = 988 Step =    8890 loss = 0.030\n",
      "Epoch = 989 Step =    8900 loss = 0.030\n",
      "Epoch = 990 Step =    8910 loss = 0.030\n",
      "Epoch = 992 Step =    8920 loss = 0.030\n",
      "Epoch = 993 Step =    8930 loss = 0.029\n",
      "Epoch = 994 Step =    8940 loss = 0.029\n",
      "Epoch = 995 Step =    8950 loss = 0.029\n",
      "Epoch = 996 Step =    8960 loss = 0.029\n",
      "Epoch = 997 Step =    8970 loss = 0.029\n",
      "Epoch = 998 Step =    8980 loss = 0.029\n",
      "Epoch = 999 Step =    8990 loss = 0.029\n",
      "Epoch = 1000 Step =    9000 loss = 0.029\n",
      "Epoch = 1002 Step =    9010 loss = 0.029\n",
      "Epoch = 1003 Step =    9020 loss = 0.029\n",
      "Epoch = 1004 Step =    9030 loss = 0.029\n",
      "Epoch = 1005 Step =    9040 loss = 0.029\n",
      "Epoch = 1006 Step =    9050 loss = 0.029\n",
      "Epoch = 1007 Step =    9060 loss = 0.029\n",
      "Epoch = 1008 Step =    9070 loss = 0.029\n",
      "Epoch = 1009 Step =    9080 loss = 0.029\n",
      "Epoch = 1010 Step =    9090 loss = 0.029\n",
      "Epoch = 1012 Step =    9100 loss = 0.029\n",
      "Epoch = 1013 Step =    9110 loss = 0.029\n",
      "Epoch = 1014 Step =    9120 loss = 0.029\n",
      "Epoch = 1015 Step =    9130 loss = 0.029\n",
      "Epoch = 1016 Step =    9140 loss = 0.029\n",
      "Epoch = 1017 Step =    9150 loss = 0.029\n",
      "Epoch = 1018 Step =    9160 loss = 0.029\n",
      "Epoch = 1019 Step =    9170 loss = 0.029\n",
      "Epoch = 1020 Step =    9180 loss = 0.029\n",
      "Epoch = 1022 Step =    9190 loss = 0.029\n",
      "Epoch = 1023 Step =    9200 loss = 0.029\n",
      "Epoch = 1024 Step =    9210 loss = 0.029\n",
      "Epoch = 1025 Step =    9220 loss = 0.029\n",
      "Epoch = 1026 Step =    9230 loss = 0.029\n",
      "Epoch = 1027 Step =    9240 loss = 0.029\n",
      "Epoch = 1028 Step =    9250 loss = 0.029\n",
      "Epoch = 1029 Step =    9260 loss = 0.029\n",
      "Epoch = 1030 Step =    9270 loss = 0.029\n",
      "Epoch = 1032 Step =    9280 loss = 0.029\n",
      "Epoch = 1033 Step =    9290 loss = 0.028\n",
      "Epoch = 1034 Step =    9300 loss = 0.028\n",
      "Epoch = 1035 Step =    9310 loss = 0.028\n",
      "Epoch = 1036 Step =    9320 loss = 0.028\n",
      "Epoch = 1037 Step =    9330 loss = 0.028\n",
      "Epoch = 1038 Step =    9340 loss = 0.028\n",
      "Epoch = 1039 Step =    9350 loss = 0.028\n",
      "Epoch = 1040 Step =    9360 loss = 0.028\n",
      "Epoch = 1042 Step =    9370 loss = 0.028\n",
      "Epoch = 1043 Step =    9380 loss = 0.028\n",
      "Epoch = 1044 Step =    9390 loss = 0.028\n",
      "Epoch = 1045 Step =    9400 loss = 0.028\n",
      "Epoch = 1046 Step =    9410 loss = 0.028\n",
      "Epoch = 1047 Step =    9420 loss = 0.028\n",
      "Epoch = 1048 Step =    9430 loss = 0.028\n",
      "Epoch = 1049 Step =    9440 loss = 0.028\n",
      "Epoch = 1050 Step =    9450 loss = 0.028\n",
      "Epoch = 1052 Step =    9460 loss = 0.028\n",
      "Epoch = 1053 Step =    9470 loss = 0.028\n",
      "Epoch = 1054 Step =    9480 loss = 0.028\n",
      "Epoch = 1055 Step =    9490 loss = 0.028\n",
      "Epoch = 1056 Step =    9500 loss = 0.028\n",
      "Epoch = 1057 Step =    9510 loss = 0.028\n",
      "Epoch = 1058 Step =    9520 loss = 0.028\n",
      "Epoch = 1059 Step =    9530 loss = 0.028\n",
      "Epoch = 1060 Step =    9540 loss = 0.028\n",
      "Epoch = 1062 Step =    9550 loss = 0.028\n",
      "Epoch = 1063 Step =    9560 loss = 0.028\n",
      "Epoch = 1064 Step =    9570 loss = 0.028\n",
      "Epoch = 1065 Step =    9580 loss = 0.028\n",
      "Epoch = 1066 Step =    9590 loss = 0.028\n",
      "Epoch = 1067 Step =    9600 loss = 0.028\n",
      "Epoch = 1068 Step =    9610 loss = 0.028\n",
      "Epoch = 1069 Step =    9620 loss = 0.028\n",
      "Epoch = 1070 Step =    9630 loss = 0.028\n",
      "Epoch = 1072 Step =    9640 loss = 0.027\n",
      "Epoch = 1073 Step =    9650 loss = 0.027\n",
      "Epoch = 1074 Step =    9660 loss = 0.027\n",
      "Epoch = 1075 Step =    9670 loss = 0.027\n",
      "Epoch = 1076 Step =    9680 loss = 0.027\n",
      "Epoch = 1077 Step =    9690 loss = 0.027\n",
      "Epoch = 1078 Step =    9700 loss = 0.027\n",
      "Epoch = 1079 Step =    9710 loss = 0.027\n",
      "Epoch = 1080 Step =    9720 loss = 0.027\n",
      "Epoch = 1082 Step =    9730 loss = 0.027\n",
      "Epoch = 1083 Step =    9740 loss = 0.027\n",
      "Epoch = 1084 Step =    9750 loss = 0.027\n",
      "Epoch = 1085 Step =    9760 loss = 0.027\n",
      "Epoch = 1086 Step =    9770 loss = 0.027\n",
      "Epoch = 1087 Step =    9780 loss = 0.027\n",
      "Epoch = 1088 Step =    9790 loss = 0.027\n",
      "Epoch = 1089 Step =    9800 loss = 0.027\n",
      "Epoch = 1090 Step =    9810 loss = 0.027\n",
      "Epoch = 1092 Step =    9820 loss = 0.027\n",
      "Epoch = 1093 Step =    9830 loss = 0.027\n",
      "Epoch = 1094 Step =    9840 loss = 0.027\n",
      "Epoch = 1095 Step =    9850 loss = 0.027\n",
      "Epoch = 1096 Step =    9860 loss = 0.027\n",
      "Epoch = 1097 Step =    9870 loss = 0.027\n",
      "Epoch = 1098 Step =    9880 loss = 0.027\n",
      "Epoch = 1099 Step =    9890 loss = 0.027\n",
      "Epoch = 1100 Step =    9900 loss = 0.027\n",
      "Epoch = 1102 Step =    9910 loss = 0.027\n",
      "Epoch = 1103 Step =    9920 loss = 0.027\n",
      "Epoch = 1104 Step =    9930 loss = 0.027\n",
      "Epoch = 1105 Step =    9940 loss = 0.027\n",
      "Epoch = 1106 Step =    9950 loss = 0.027\n",
      "Epoch = 1107 Step =    9960 loss = 0.027\n",
      "Epoch = 1108 Step =    9970 loss = 0.027\n",
      "Epoch = 1109 Step =    9980 loss = 0.027\n",
      "Epoch = 1110 Step =    9990 loss = 0.027\n",
      "Epoch = 1112 Step =   10000 loss = 0.027\n",
      "Epoch = 1113 Step =   10010 loss = 0.027\n",
      "Epoch = 1114 Step =   10020 loss = 0.027\n",
      "Epoch = 1115 Step =   10030 loss = 0.027\n",
      "Epoch = 1116 Step =   10040 loss = 0.027\n",
      "Epoch = 1117 Step =   10050 loss = 0.027\n",
      "Epoch = 1118 Step =   10060 loss = 0.026\n",
      "Epoch = 1119 Step =   10070 loss = 0.026\n",
      "Epoch = 1120 Step =   10080 loss = 0.026\n",
      "Epoch = 1122 Step =   10090 loss = 0.026\n",
      "Epoch = 1123 Step =   10100 loss = 0.026\n",
      "Epoch = 1124 Step =   10110 loss = 0.026\n",
      "Epoch = 1125 Step =   10120 loss = 0.026\n",
      "Epoch = 1126 Step =   10130 loss = 0.026\n",
      "Epoch = 1127 Step =   10140 loss = 0.026\n",
      "Epoch = 1128 Step =   10150 loss = 0.026\n",
      "Epoch = 1129 Step =   10160 loss = 0.026\n",
      "Epoch = 1130 Step =   10170 loss = 0.026\n",
      "Epoch = 1132 Step =   10180 loss = 0.026\n",
      "Epoch = 1133 Step =   10190 loss = 0.026\n",
      "Epoch = 1134 Step =   10200 loss = 0.026\n",
      "Epoch = 1135 Step =   10210 loss = 0.026\n",
      "Epoch = 1136 Step =   10220 loss = 0.026\n",
      "Epoch = 1137 Step =   10230 loss = 0.026\n",
      "Epoch = 1138 Step =   10240 loss = 0.026\n",
      "Epoch = 1139 Step =   10250 loss = 0.026\n",
      "Epoch = 1140 Step =   10260 loss = 0.026\n",
      "Epoch = 1142 Step =   10270 loss = 0.026\n",
      "Epoch = 1143 Step =   10280 loss = 0.026\n",
      "Epoch = 1144 Step =   10290 loss = 0.026\n",
      "Epoch = 1145 Step =   10300 loss = 0.026\n",
      "Epoch = 1146 Step =   10310 loss = 0.026\n",
      "Epoch = 1147 Step =   10320 loss = 0.026\n",
      "Epoch = 1148 Step =   10330 loss = 0.026\n",
      "Epoch = 1149 Step =   10340 loss = 0.026\n",
      "Epoch = 1150 Step =   10350 loss = 0.026\n",
      "Epoch = 1152 Step =   10360 loss = 0.026\n",
      "Epoch = 1153 Step =   10370 loss = 0.026\n",
      "Epoch = 1154 Step =   10380 loss = 0.026\n",
      "Epoch = 1155 Step =   10390 loss = 0.026\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models/model.ckpt\n",
      "INFO:tensorflow:model/global_step/sec: 17.3753\n",
      "Epoch = 1156 Step =   10400 loss = 0.026\n",
      "Epoch = 1157 Step =   10410 loss = 0.026\n",
      "Epoch = 1158 Step =   10420 loss = 0.026\n",
      "Epoch = 1159 Step =   10430 loss = 0.026\n",
      "Epoch = 1160 Step =   10440 loss = 0.026\n",
      "Epoch = 1162 Step =   10450 loss = 0.026\n",
      "Epoch = 1163 Step =   10460 loss = 0.026\n",
      "Epoch = 1164 Step =   10470 loss = 0.026\n",
      "Epoch = 1165 Step =   10480 loss = 0.026\n",
      "Epoch = 1166 Step =   10490 loss = 0.026\n",
      "Epoch = 1167 Step =   10500 loss = 0.025\n",
      "Epoch = 1168 Step =   10510 loss = 0.025\n",
      "Epoch = 1169 Step =   10520 loss = 0.025\n",
      "Epoch = 1170 Step =   10530 loss = 0.025\n",
      "Epoch = 1172 Step =   10540 loss = 0.025\n",
      "Epoch = 1173 Step =   10550 loss = 0.025\n",
      "Epoch = 1174 Step =   10560 loss = 0.025\n",
      "Epoch = 1175 Step =   10570 loss = 0.025\n",
      "Epoch = 1176 Step =   10580 loss = 0.025\n",
      "Epoch = 1177 Step =   10590 loss = 0.025\n",
      "Epoch = 1178 Step =   10600 loss = 0.025\n",
      "Epoch = 1179 Step =   10610 loss = 0.025\n",
      "Epoch = 1180 Step =   10620 loss = 0.025\n",
      "Epoch = 1182 Step =   10630 loss = 0.025\n",
      "Epoch = 1183 Step =   10640 loss = 0.025\n",
      "Epoch = 1184 Step =   10650 loss = 0.025\n",
      "Epoch = 1185 Step =   10660 loss = 0.025\n",
      "Epoch = 1186 Step =   10670 loss = 0.025\n",
      "Epoch = 1187 Step =   10680 loss = 0.025\n",
      "Epoch = 1188 Step =   10690 loss = 0.025\n",
      "Epoch = 1189 Step =   10700 loss = 0.025\n",
      "Epoch = 1190 Step =   10710 loss = 0.025\n",
      "Epoch = 1192 Step =   10720 loss = 0.025\n",
      "Epoch = 1193 Step =   10730 loss = 0.025\n",
      "Epoch = 1194 Step =   10740 loss = 0.025\n",
      "Epoch = 1195 Step =   10750 loss = 0.025\n",
      "Epoch = 1196 Step =   10760 loss = 0.025\n",
      "Epoch = 1197 Step =   10770 loss = 0.025\n",
      "Epoch = 1198 Step =   10780 loss = 0.025\n",
      "Epoch = 1199 Step =   10790 loss = 0.025\n",
      "Epoch = 1200 Step =   10800 loss = 0.025\n",
      "Epoch = 1202 Step =   10810 loss = 0.025\n",
      "Epoch = 1203 Step =   10820 loss = 0.025\n",
      "Epoch = 1204 Step =   10830 loss = 0.025\n",
      "Epoch = 1205 Step =   10840 loss = 0.025\n",
      "Epoch = 1206 Step =   10850 loss = 0.025\n",
      "Epoch = 1207 Step =   10860 loss = 0.025\n",
      "Epoch = 1208 Step =   10870 loss = 0.025\n",
      "Epoch = 1209 Step =   10880 loss = 0.025\n",
      "Epoch = 1210 Step =   10890 loss = 0.025\n",
      "Epoch = 1212 Step =   10900 loss = 0.025\n",
      "Epoch = 1213 Step =   10910 loss = 0.025\n",
      "Epoch = 1214 Step =   10920 loss = 0.025\n",
      "Epoch = 1215 Step =   10930 loss = 0.025\n",
      "Epoch = 1216 Step =   10940 loss = 0.025\n",
      "Epoch = 1217 Step =   10950 loss = 0.025\n",
      "Epoch = 1218 Step =   10960 loss = 0.024\n",
      "Epoch = 1219 Step =   10970 loss = 0.024\n",
      "Epoch = 1220 Step =   10980 loss = 0.024\n",
      "Epoch = 1222 Step =   10990 loss = 0.024\n",
      "Epoch = 1223 Step =   11000 loss = 0.024\n",
      "Epoch = 1224 Step =   11010 loss = 0.024\n",
      "Epoch = 1225 Step =   11020 loss = 0.024\n",
      "Epoch = 1226 Step =   11030 loss = 0.024\n",
      "Epoch = 1227 Step =   11040 loss = 0.024\n",
      "Epoch = 1228 Step =   11050 loss = 0.024\n",
      "Epoch = 1229 Step =   11060 loss = 0.024\n",
      "Epoch = 1230 Step =   11070 loss = 0.024\n",
      "Epoch = 1232 Step =   11080 loss = 0.024\n",
      "Epoch = 1233 Step =   11090 loss = 0.024\n",
      "Epoch = 1234 Step =   11100 loss = 0.024\n",
      "Epoch = 1235 Step =   11110 loss = 0.024\n",
      "Epoch = 1236 Step =   11120 loss = 0.024\n",
      "Epoch = 1237 Step =   11130 loss = 0.024\n",
      "Epoch = 1238 Step =   11140 loss = 0.024\n",
      "Epoch = 1239 Step =   11150 loss = 0.024\n",
      "Epoch = 1240 Step =   11160 loss = 0.024\n",
      "Epoch = 1242 Step =   11170 loss = 0.024\n",
      "Epoch = 1243 Step =   11180 loss = 0.024\n",
      "Epoch = 1244 Step =   11190 loss = 0.024\n",
      "Epoch = 1245 Step =   11200 loss = 0.024\n",
      "Epoch = 1246 Step =   11210 loss = 0.024\n",
      "Epoch = 1247 Step =   11220 loss = 0.024\n",
      "Epoch = 1248 Step =   11230 loss = 0.024\n",
      "Epoch = 1249 Step =   11240 loss = 0.024\n",
      "Epoch = 1250 Step =   11250 loss = 0.024\n",
      "Epoch = 1252 Step =   11260 loss = 0.024\n",
      "Epoch = 1253 Step =   11270 loss = 0.024\n",
      "Epoch = 1254 Step =   11280 loss = 0.024\n",
      "Epoch = 1255 Step =   11290 loss = 0.024\n",
      "Epoch = 1256 Step =   11300 loss = 0.024\n",
      "Epoch = 1257 Step =   11310 loss = 0.024\n",
      "Epoch = 1258 Step =   11320 loss = 0.024\n",
      "Epoch = 1259 Step =   11330 loss = 0.024\n",
      "Epoch = 1260 Step =   11340 loss = 0.024\n",
      "Epoch = 1262 Step =   11350 loss = 0.024\n",
      "Epoch = 1263 Step =   11360 loss = 0.024\n",
      "Epoch = 1264 Step =   11370 loss = 0.024\n",
      "Epoch = 1265 Step =   11380 loss = 0.024\n",
      "Epoch = 1266 Step =   11390 loss = 0.024\n",
      "Epoch = 1267 Step =   11400 loss = 0.024\n",
      "Epoch = 1268 Step =   11410 loss = 0.024\n",
      "Epoch = 1269 Step =   11420 loss = 0.024\n",
      "Epoch = 1270 Step =   11430 loss = 0.024\n",
      "Epoch = 1272 Step =   11440 loss = 0.024\n",
      "Epoch = 1273 Step =   11450 loss = 0.024\n",
      "Epoch = 1274 Step =   11460 loss = 0.024\n",
      "Epoch = 1275 Step =   11470 loss = 0.024\n",
      "Epoch = 1276 Step =   11480 loss = 0.024\n",
      "Epoch = 1277 Step =   11490 loss = 0.024\n",
      "Epoch = 1278 Step =   11500 loss = 0.023\n",
      "Epoch = 1279 Step =   11510 loss = 0.023\n",
      "Epoch = 1280 Step =   11520 loss = 0.023\n",
      "Epoch = 1282 Step =   11530 loss = 0.023\n",
      "Epoch = 1283 Step =   11540 loss = 0.023\n",
      "Epoch = 1284 Step =   11550 loss = 0.023\n",
      "Epoch = 1285 Step =   11560 loss = 0.023\n",
      "Epoch = 1286 Step =   11570 loss = 0.023\n",
      "Epoch = 1287 Step =   11580 loss = 0.023\n",
      "Epoch = 1288 Step =   11590 loss = 0.023\n",
      "Epoch = 1289 Step =   11600 loss = 0.023\n",
      "Epoch = 1290 Step =   11610 loss = 0.023\n",
      "Epoch = 1292 Step =   11620 loss = 0.023\n",
      "Epoch = 1293 Step =   11630 loss = 0.023\n",
      "Epoch = 1294 Step =   11640 loss = 0.023\n",
      "Epoch = 1295 Step =   11650 loss = 0.023\n",
      "Epoch = 1296 Step =   11660 loss = 0.023\n",
      "Epoch = 1297 Step =   11670 loss = 0.023\n",
      "Epoch = 1298 Step =   11680 loss = 0.023\n",
      "Epoch = 1299 Step =   11690 loss = 0.023\n",
      "Epoch = 1300 Step =   11700 loss = 0.023\n",
      "Epoch = 1302 Step =   11710 loss = 0.023\n",
      "Epoch = 1303 Step =   11720 loss = 0.023\n",
      "Epoch = 1304 Step =   11730 loss = 0.023\n",
      "Epoch = 1305 Step =   11740 loss = 0.023\n",
      "Epoch = 1306 Step =   11750 loss = 0.023\n",
      "Epoch = 1307 Step =   11760 loss = 0.023\n",
      "Epoch = 1308 Step =   11770 loss = 0.023\n",
      "Epoch = 1309 Step =   11780 loss = 0.023\n",
      "Epoch = 1310 Step =   11790 loss = 0.023\n",
      "Epoch = 1312 Step =   11800 loss = 0.023\n",
      "Epoch = 1313 Step =   11810 loss = 0.023\n",
      "Epoch = 1314 Step =   11820 loss = 0.023\n",
      "Epoch = 1315 Step =   11830 loss = 0.023\n",
      "Epoch = 1316 Step =   11840 loss = 0.023\n",
      "Epoch = 1317 Step =   11850 loss = 0.023\n",
      "Epoch = 1318 Step =   11860 loss = 0.023\n",
      "Epoch = 1319 Step =   11870 loss = 0.023\n",
      "Epoch = 1320 Step =   11880 loss = 0.023\n",
      "Epoch = 1322 Step =   11890 loss = 0.023\n",
      "Epoch = 1323 Step =   11900 loss = 0.023\n",
      "Epoch = 1324 Step =   11910 loss = 0.023\n",
      "Epoch = 1325 Step =   11920 loss = 0.023\n",
      "Epoch = 1326 Step =   11930 loss = 0.023\n",
      "Epoch = 1327 Step =   11940 loss = 0.023\n",
      "Epoch = 1328 Step =   11950 loss = 0.023\n",
      "Epoch = 1329 Step =   11960 loss = 0.023\n",
      "Epoch = 1330 Step =   11970 loss = 0.023\n",
      "Epoch = 1332 Step =   11980 loss = 0.023\n",
      "Epoch = 1333 Step =   11990 loss = 0.023\n",
      "Epoch = 1334 Step =   12000 loss = 0.023\n",
      "Epoch = 1335 Step =   12010 loss = 0.023\n",
      "Epoch = 1336 Step =   12020 loss = 0.023\n",
      "Epoch = 1337 Step =   12030 loss = 0.023\n",
      "Epoch = 1338 Step =   12040 loss = 0.023\n",
      "Epoch = 1339 Step =   12050 loss = 0.023\n",
      "Epoch = 1340 Step =   12060 loss = 0.022\n",
      "Epoch = 1342 Step =   12070 loss = 0.022\n",
      "Epoch = 1343 Step =   12080 loss = 0.022\n",
      "Epoch = 1344 Step =   12090 loss = 0.022\n",
      "Epoch = 1345 Step =   12100 loss = 0.022\n",
      "Epoch = 1346 Step =   12110 loss = 0.022\n",
      "Epoch = 1347 Step =   12120 loss = 0.022\n",
      "Epoch = 1348 Step =   12130 loss = 0.022\n",
      "Epoch = 1349 Step =   12140 loss = 0.022\n",
      "Epoch = 1350 Step =   12150 loss = 0.022\n",
      "Epoch = 1352 Step =   12160 loss = 0.022\n",
      "Epoch = 1353 Step =   12170 loss = 0.022\n",
      "Epoch = 1354 Step =   12180 loss = 0.022\n",
      "Epoch = 1355 Step =   12190 loss = 0.022\n",
      "Epoch = 1356 Step =   12200 loss = 0.022\n",
      "Epoch = 1357 Step =   12210 loss = 0.022\n",
      "Epoch = 1358 Step =   12220 loss = 0.022\n",
      "Epoch = 1359 Step =   12230 loss = 0.022\n",
      "Epoch = 1360 Step =   12240 loss = 0.022\n",
      "Epoch = 1362 Step =   12250 loss = 0.022\n",
      "Epoch = 1363 Step =   12260 loss = 0.022\n",
      "Epoch = 1364 Step =   12270 loss = 0.022\n",
      "Epoch = 1365 Step =   12280 loss = 0.022\n",
      "Epoch = 1366 Step =   12290 loss = 0.022\n",
      "Epoch = 1367 Step =   12300 loss = 0.022\n",
      "Epoch = 1368 Step =   12310 loss = 0.022\n",
      "Epoch = 1369 Step =   12320 loss = 0.022\n",
      "Epoch = 1370 Step =   12330 loss = 0.022\n",
      "Epoch = 1372 Step =   12340 loss = 0.022\n",
      "Epoch = 1373 Step =   12350 loss = 0.022\n",
      "Epoch = 1374 Step =   12360 loss = 0.022\n",
      "Epoch = 1375 Step =   12370 loss = 0.022\n",
      "Epoch = 1376 Step =   12380 loss = 0.022\n",
      "Epoch = 1377 Step =   12390 loss = 0.022\n",
      "Epoch = 1378 Step =   12400 loss = 0.022\n",
      "Epoch = 1379 Step =   12410 loss = 0.022\n",
      "Epoch = 1380 Step =   12420 loss = 0.022\n",
      "Epoch = 1382 Step =   12430 loss = 0.022\n",
      "Epoch = 1383 Step =   12440 loss = 0.022\n",
      "Epoch = 1384 Step =   12450 loss = 0.022\n",
      "Epoch = 1385 Step =   12460 loss = 0.022\n",
      "Epoch = 1386 Step =   12470 loss = 0.022\n",
      "INFO:tensorflow:model/global_step/sec: 17.2997\n",
      "Epoch = 1387 Step =   12480 loss = 0.022\n",
      "Epoch = 1388 Step =   12490 loss = 0.022\n",
      "Epoch = 1389 Step =   12500 loss = 0.022\n",
      "Epoch = 1390 Step =   12510 loss = 0.022\n",
      "Epoch = 1392 Step =   12520 loss = 0.022\n",
      "Epoch = 1393 Step =   12530 loss = 0.022\n",
      "Epoch = 1394 Step =   12540 loss = 0.022\n",
      "Epoch = 1395 Step =   12550 loss = 0.022\n",
      "Epoch = 1396 Step =   12560 loss = 0.022\n",
      "Epoch = 1397 Step =   12570 loss = 0.022\n",
      "Epoch = 1398 Step =   12580 loss = 0.022\n",
      "Epoch = 1399 Step =   12590 loss = 0.022\n",
      "Epoch = 1400 Step =   12600 loss = 0.022\n",
      "Epoch = 1402 Step =   12610 loss = 0.022\n",
      "Epoch = 1403 Step =   12620 loss = 0.022\n",
      "Epoch = 1404 Step =   12630 loss = 0.022\n",
      "Epoch = 1405 Step =   12640 loss = 0.021\n",
      "Epoch = 1406 Step =   12650 loss = 0.021\n",
      "Epoch = 1407 Step =   12660 loss = 0.021\n",
      "Epoch = 1408 Step =   12670 loss = 0.021\n",
      "Epoch = 1409 Step =   12680 loss = 0.021\n",
      "Epoch = 1410 Step =   12690 loss = 0.021\n",
      "Epoch = 1412 Step =   12700 loss = 0.021\n",
      "Epoch = 1413 Step =   12710 loss = 0.021\n",
      "Epoch = 1414 Step =   12720 loss = 0.021\n",
      "Epoch = 1415 Step =   12730 loss = 0.021\n",
      "Epoch = 1416 Step =   12740 loss = 0.021\n",
      "Epoch = 1417 Step =   12750 loss = 0.021\n",
      "Epoch = 1418 Step =   12760 loss = 0.021\n",
      "Epoch = 1419 Step =   12770 loss = 0.021\n",
      "Epoch = 1420 Step =   12780 loss = 0.021\n",
      "Epoch = 1422 Step =   12790 loss = 0.021\n",
      "Epoch = 1423 Step =   12800 loss = 0.021\n",
      "Epoch = 1424 Step =   12810 loss = 0.021\n",
      "Epoch = 1425 Step =   12820 loss = 0.021\n",
      "Epoch = 1426 Step =   12830 loss = 0.021\n",
      "Epoch = 1427 Step =   12840 loss = 0.021\n",
      "Epoch = 1428 Step =   12850 loss = 0.021\n",
      "Epoch = 1429 Step =   12860 loss = 0.021\n",
      "Epoch = 1430 Step =   12870 loss = 0.021\n",
      "Epoch = 1432 Step =   12880 loss = 0.021\n",
      "Epoch = 1433 Step =   12890 loss = 0.021\n",
      "Epoch = 1434 Step =   12900 loss = 0.021\n",
      "Epoch = 1435 Step =   12910 loss = 0.021\n",
      "Epoch = 1436 Step =   12920 loss = 0.021\n",
      "Epoch = 1437 Step =   12930 loss = 0.021\n",
      "Epoch = 1438 Step =   12940 loss = 0.021\n",
      "Epoch = 1439 Step =   12950 loss = 0.021\n",
      "Epoch = 1440 Step =   12960 loss = 0.021\n",
      "Epoch = 1442 Step =   12970 loss = 0.021\n",
      "Epoch = 1443 Step =   12980 loss = 0.021\n",
      "Epoch = 1444 Step =   12990 loss = 0.021\n",
      "Epoch = 1445 Step =   13000 loss = 0.021\n",
      "Epoch = 1446 Step =   13010 loss = 0.021\n",
      "Epoch = 1447 Step =   13020 loss = 0.021\n",
      "Epoch = 1448 Step =   13030 loss = 0.021\n",
      "Epoch = 1449 Step =   13040 loss = 0.021\n",
      "Epoch = 1450 Step =   13050 loss = 0.021\n",
      "Epoch = 1452 Step =   13060 loss = 0.021\n",
      "Epoch = 1453 Step =   13070 loss = 0.021\n",
      "Epoch = 1454 Step =   13080 loss = 0.021\n",
      "Epoch = 1455 Step =   13090 loss = 0.021\n",
      "Epoch = 1456 Step =   13100 loss = 0.021\n",
      "Epoch = 1457 Step =   13110 loss = 0.021\n",
      "Epoch = 1458 Step =   13120 loss = 0.021\n",
      "Epoch = 1459 Step =   13130 loss = 0.021\n",
      "Epoch = 1460 Step =   13140 loss = 0.021\n",
      "Epoch = 1462 Step =   13150 loss = 0.021\n",
      "Epoch = 1463 Step =   13160 loss = 0.021\n",
      "Epoch = 1464 Step =   13170 loss = 0.021\n",
      "Epoch = 1465 Step =   13180 loss = 0.021\n",
      "Epoch = 1466 Step =   13190 loss = 0.021\n",
      "Epoch = 1467 Step =   13200 loss = 0.021\n",
      "Epoch = 1468 Step =   13210 loss = 0.021\n",
      "Epoch = 1469 Step =   13220 loss = 0.021\n",
      "Epoch = 1470 Step =   13230 loss = 0.021\n",
      "Epoch = 1472 Step =   13240 loss = 0.021\n",
      "Epoch = 1473 Step =   13250 loss = 0.021\n",
      "Epoch = 1474 Step =   13260 loss = 0.021\n",
      "Epoch = 1475 Step =   13270 loss = 0.021\n",
      "Epoch = 1476 Step =   13280 loss = 0.021\n",
      "Epoch = 1477 Step =   13290 loss = 0.021\n",
      "Epoch = 1478 Step =   13300 loss = 0.021\n",
      "Epoch = 1479 Step =   13310 loss = 0.021\n",
      "Epoch = 1480 Step =   13320 loss = 0.020\n",
      "Epoch = 1482 Step =   13330 loss = 0.020\n",
      "Epoch = 1483 Step =   13340 loss = 0.020\n",
      "Epoch = 1484 Step =   13350 loss = 0.020\n",
      "Epoch = 1485 Step =   13360 loss = 0.020\n",
      "Epoch = 1486 Step =   13370 loss = 0.020\n",
      "Epoch = 1487 Step =   13380 loss = 0.020\n",
      "Epoch = 1488 Step =   13390 loss = 0.020\n",
      "Epoch = 1489 Step =   13400 loss = 0.020\n",
      "Epoch = 1490 Step =   13410 loss = 0.020\n",
      "Epoch = 1492 Step =   13420 loss = 0.020\n",
      "Epoch = 1493 Step =   13430 loss = 0.020\n",
      "Epoch = 1494 Step =   13440 loss = 0.020\n",
      "Epoch = 1495 Step =   13450 loss = 0.020\n",
      "Epoch = 1496 Step =   13460 loss = 0.020\n",
      "Epoch = 1497 Step =   13470 loss = 0.020\n",
      "Epoch = 1498 Step =   13480 loss = 0.020\n",
      "Epoch = 1499 Step =   13490 loss = 0.020\n",
      "Epoch = 1500 Step =   13500 loss = 0.020\n",
      "Epoch = 1502 Step =   13510 loss = 0.020\n",
      "Epoch = 1503 Step =   13520 loss = 0.020\n",
      "Epoch = 1504 Step =   13530 loss = 0.020\n",
      "Epoch = 1505 Step =   13540 loss = 0.020\n",
      "Epoch = 1506 Step =   13550 loss = 0.020\n",
      "Epoch = 1507 Step =   13560 loss = 0.020\n",
      "Epoch = 1508 Step =   13570 loss = 0.020\n",
      "Epoch = 1509 Step =   13580 loss = 0.020\n",
      "Epoch = 1510 Step =   13590 loss = 0.020\n",
      "Epoch = 1512 Step =   13600 loss = 0.020\n",
      "Epoch = 1513 Step =   13610 loss = 0.020\n",
      "Epoch = 1514 Step =   13620 loss = 0.020\n",
      "Epoch = 1515 Step =   13630 loss = 0.020\n",
      "Epoch = 1516 Step =   13640 loss = 0.020\n",
      "Epoch = 1517 Step =   13650 loss = 0.020\n",
      "Epoch = 1518 Step =   13660 loss = 0.020\n",
      "Epoch = 1519 Step =   13670 loss = 0.020\n",
      "Epoch = 1520 Step =   13680 loss = 0.020\n",
      "Epoch = 1522 Step =   13690 loss = 0.020\n",
      "Epoch = 1523 Step =   13700 loss = 0.020\n",
      "Epoch = 1524 Step =   13710 loss = 0.020\n",
      "Epoch = 1525 Step =   13720 loss = 0.020\n",
      "Epoch = 1526 Step =   13730 loss = 0.020\n",
      "Epoch = 1527 Step =   13740 loss = 0.020\n",
      "Epoch = 1528 Step =   13750 loss = 0.020\n",
      "Epoch = 1529 Step =   13760 loss = 0.020\n",
      "Epoch = 1530 Step =   13770 loss = 0.020\n",
      "Epoch = 1532 Step =   13780 loss = 0.020\n",
      "Epoch = 1533 Step =   13790 loss = 0.020\n",
      "Epoch = 1534 Step =   13800 loss = 0.020\n",
      "Epoch = 1535 Step =   13810 loss = 0.020\n",
      "Epoch = 1536 Step =   13820 loss = 0.020\n",
      "Epoch = 1537 Step =   13830 loss = 0.020\n",
      "Epoch = 1538 Step =   13840 loss = 0.020\n",
      "Epoch = 1539 Step =   13850 loss = 0.020\n",
      "Epoch = 1540 Step =   13860 loss = 0.020\n",
      "Epoch = 1542 Step =   13870 loss = 0.020\n",
      "Epoch = 1543 Step =   13880 loss = 0.020\n",
      "Epoch = 1544 Step =   13890 loss = 0.020\n",
      "Epoch = 1545 Step =   13900 loss = 0.020\n",
      "Epoch = 1546 Step =   13910 loss = 0.020\n",
      "Epoch = 1547 Step =   13920 loss = 0.020\n",
      "Epoch = 1548 Step =   13930 loss = 0.020\n",
      "Epoch = 1549 Step =   13940 loss = 0.020\n",
      "Epoch = 1550 Step =   13950 loss = 0.020\n",
      "Epoch = 1552 Step =   13960 loss = 0.020\n",
      "Epoch = 1553 Step =   13970 loss = 0.020\n",
      "Epoch = 1554 Step =   13980 loss = 0.020\n",
      "Epoch = 1555 Step =   13990 loss = 0.020\n",
      "Epoch = 1556 Step =   14000 loss = 0.020\n",
      "Epoch = 1557 Step =   14010 loss = 0.020\n",
      "Epoch = 1558 Step =   14020 loss = 0.020\n",
      "Epoch = 1559 Step =   14030 loss = 0.020\n",
      "Epoch = 1560 Step =   14040 loss = 0.020\n",
      "Epoch = 1562 Step =   14050 loss = 0.020\n",
      "Epoch = 1563 Step =   14060 loss = 0.020\n",
      "Epoch = 1564 Step =   14070 loss = 0.020\n",
      "Epoch = 1565 Step =   14080 loss = 0.020\n",
      "Epoch = 1566 Step =   14090 loss = 0.020\n",
      "Epoch = 1567 Step =   14100 loss = 0.020\n",
      "Epoch = 1568 Step =   14110 loss = 0.019\n",
      "Epoch = 1569 Step =   14120 loss = 0.019\n",
      "Epoch = 1570 Step =   14130 loss = 0.019\n",
      "Epoch = 1572 Step =   14140 loss = 0.019\n",
      "Epoch = 1573 Step =   14150 loss = 0.019\n",
      "Epoch = 1574 Step =   14160 loss = 0.019\n",
      "Epoch = 1575 Step =   14170 loss = 0.019\n",
      "Epoch = 1576 Step =   14180 loss = 0.019\n",
      "Epoch = 1577 Step =   14190 loss = 0.019\n",
      "Epoch = 1578 Step =   14200 loss = 0.019\n",
      "Epoch = 1579 Step =   14210 loss = 0.019\n",
      "Epoch = 1580 Step =   14220 loss = 0.019\n",
      "Epoch = 1582 Step =   14230 loss = 0.019\n",
      "Epoch = 1583 Step =   14240 loss = 0.019\n",
      "Epoch = 1584 Step =   14250 loss = 0.019\n",
      "Epoch = 1585 Step =   14260 loss = 0.019\n",
      "Epoch = 1586 Step =   14270 loss = 0.019\n",
      "Epoch = 1587 Step =   14280 loss = 0.019\n",
      "Epoch = 1588 Step =   14290 loss = 0.019\n",
      "Epoch = 1589 Step =   14300 loss = 0.019\n",
      "Epoch = 1590 Step =   14310 loss = 0.019\n",
      "Epoch = 1592 Step =   14320 loss = 0.019\n",
      "Epoch = 1593 Step =   14330 loss = 0.019\n",
      "Epoch = 1594 Step =   14340 loss = 0.019\n",
      "Epoch = 1595 Step =   14350 loss = 0.019\n",
      "Epoch = 1596 Step =   14360 loss = 0.019\n",
      "Epoch = 1597 Step =   14370 loss = 0.019\n",
      "Epoch = 1598 Step =   14380 loss = 0.019\n",
      "Epoch = 1599 Step =   14390 loss = 0.019\n",
      "Epoch = 1600 Step =   14400 loss = 0.019\n",
      "Epoch = 1602 Step =   14410 loss = 0.019\n",
      "Epoch = 1603 Step =   14420 loss = 0.019\n",
      "Epoch = 1604 Step =   14430 loss = 0.019\n",
      "Epoch = 1605 Step =   14440 loss = 0.019\n",
      "Epoch = 1606 Step =   14450 loss = 0.019\n",
      "Epoch = 1607 Step =   14460 loss = 0.019\n",
      "Epoch = 1608 Step =   14470 loss = 0.019\n",
      "Epoch = 1609 Step =   14480 loss = 0.019\n",
      "Epoch = 1610 Step =   14490 loss = 0.019\n",
      "Epoch = 1612 Step =   14500 loss = 0.019\n",
      "Epoch = 1613 Step =   14510 loss = 0.019\n",
      "Epoch = 1614 Step =   14520 loss = 0.019\n",
      "Epoch = 1615 Step =   14530 loss = 0.019\n",
      "Epoch = 1616 Step =   14540 loss = 0.019\n",
      "INFO:tensorflow:model/global_step/sec: 17.2499\n",
      "Epoch = 1617 Step =   14550 loss = 0.019\n",
      "Epoch = 1618 Step =   14560 loss = 0.019\n",
      "Epoch = 1619 Step =   14570 loss = 0.019\n",
      "Epoch = 1620 Step =   14580 loss = 0.019\n",
      "Epoch = 1622 Step =   14590 loss = 0.019\n",
      "Epoch = 1623 Step =   14600 loss = 0.019\n",
      "Epoch = 1624 Step =   14610 loss = 0.019\n",
      "Epoch = 1625 Step =   14620 loss = 0.019\n",
      "Epoch = 1626 Step =   14630 loss = 0.019\n",
      "Epoch = 1627 Step =   14640 loss = 0.019\n",
      "Epoch = 1628 Step =   14650 loss = 0.019\n",
      "Epoch = 1629 Step =   14660 loss = 0.019\n",
      "Epoch = 1630 Step =   14670 loss = 0.019\n",
      "Epoch = 1632 Step =   14680 loss = 0.019\n",
      "Epoch = 1633 Step =   14690 loss = 0.019\n",
      "Epoch = 1634 Step =   14700 loss = 0.019\n",
      "Epoch = 1635 Step =   14710 loss = 0.019\n",
      "Epoch = 1636 Step =   14720 loss = 0.019\n",
      "Epoch = 1637 Step =   14730 loss = 0.019\n",
      "Epoch = 1638 Step =   14740 loss = 0.019\n",
      "Epoch = 1639 Step =   14750 loss = 0.019\n",
      "Epoch = 1640 Step =   14760 loss = 0.019\n",
      "Epoch = 1642 Step =   14770 loss = 0.019\n",
      "Epoch = 1643 Step =   14780 loss = 0.019\n",
      "Epoch = 1644 Step =   14790 loss = 0.019\n",
      "Epoch = 1645 Step =   14800 loss = 0.019\n",
      "Epoch = 1646 Step =   14810 loss = 0.019\n",
      "Epoch = 1647 Step =   14820 loss = 0.019\n",
      "Epoch = 1648 Step =   14830 loss = 0.019\n",
      "Epoch = 1649 Step =   14840 loss = 0.019\n",
      "Epoch = 1650 Step =   14850 loss = 0.019\n",
      "Epoch = 1652 Step =   14860 loss = 0.019\n",
      "Epoch = 1653 Step =   14870 loss = 0.019\n",
      "Epoch = 1654 Step =   14880 loss = 0.019\n",
      "Epoch = 1655 Step =   14890 loss = 0.019\n",
      "Epoch = 1656 Step =   14900 loss = 0.019\n",
      "Epoch = 1657 Step =   14910 loss = 0.018\n",
      "Epoch = 1658 Step =   14920 loss = 0.018\n",
      "Epoch = 1659 Step =   14930 loss = 0.018\n",
      "Epoch = 1660 Step =   14940 loss = 0.018\n",
      "Epoch = 1662 Step =   14950 loss = 0.018\n",
      "Epoch = 1663 Step =   14960 loss = 0.018\n",
      "Epoch = 1664 Step =   14970 loss = 0.018\n",
      "Epoch = 1665 Step =   14980 loss = 0.018\n",
      "Epoch = 1666 Step =   14990 loss = 0.018\n",
      "Epoch = 1667 Step =   15000 loss = 0.018\n",
      "Epoch = 1668 Step =   15010 loss = 0.018\n",
      "Epoch = 1669 Step =   15020 loss = 0.018\n",
      "Epoch = 1670 Step =   15030 loss = 0.018\n",
      "Epoch = 1672 Step =   15040 loss = 0.018\n",
      "Epoch = 1673 Step =   15050 loss = 0.018\n",
      "Epoch = 1674 Step =   15060 loss = 0.018\n",
      "Epoch = 1675 Step =   15070 loss = 0.018\n",
      "Epoch = 1676 Step =   15080 loss = 0.018\n",
      "Epoch = 1677 Step =   15090 loss = 0.018\n",
      "Epoch = 1678 Step =   15100 loss = 0.018\n",
      "Epoch = 1679 Step =   15110 loss = 0.018\n",
      "Epoch = 1680 Step =   15120 loss = 0.018\n",
      "Epoch = 1682 Step =   15130 loss = 0.018\n",
      "Epoch = 1683 Step =   15140 loss = 0.018\n",
      "Epoch = 1684 Step =   15150 loss = 0.018\n",
      "Epoch = 1685 Step =   15160 loss = 0.018\n",
      "Epoch = 1686 Step =   15170 loss = 0.018\n",
      "Epoch = 1687 Step =   15180 loss = 0.018\n",
      "Epoch = 1688 Step =   15190 loss = 0.018\n",
      "Epoch = 1689 Step =   15200 loss = 0.018\n",
      "Epoch = 1690 Step =   15210 loss = 0.018\n",
      "Epoch = 1692 Step =   15220 loss = 0.018\n",
      "Epoch = 1693 Step =   15230 loss = 0.018\n",
      "Epoch = 1694 Step =   15240 loss = 0.018\n",
      "Epoch = 1695 Step =   15250 loss = 0.018\n",
      "Epoch = 1696 Step =   15260 loss = 0.018\n",
      "Epoch = 1697 Step =   15270 loss = 0.018\n",
      "Epoch = 1698 Step =   15280 loss = 0.018\n",
      "Epoch = 1699 Step =   15290 loss = 0.018\n",
      "Epoch = 1700 Step =   15300 loss = 0.018\n",
      "Epoch = 1702 Step =   15310 loss = 0.018\n",
      "Epoch = 1703 Step =   15320 loss = 0.018\n",
      "Epoch = 1704 Step =   15330 loss = 0.018\n",
      "Epoch = 1705 Step =   15340 loss = 0.018\n",
      "Epoch = 1706 Step =   15350 loss = 0.018\n",
      "Epoch = 1707 Step =   15360 loss = 0.018\n",
      "Epoch = 1708 Step =   15370 loss = 0.018\n",
      "Epoch = 1709 Step =   15380 loss = 0.018\n",
      "Epoch = 1710 Step =   15390 loss = 0.018\n",
      "Epoch = 1712 Step =   15400 loss = 0.018\n",
      "Epoch = 1713 Step =   15410 loss = 0.018\n",
      "Epoch = 1714 Step =   15420 loss = 0.018\n",
      "Epoch = 1715 Step =   15430 loss = 0.018\n",
      "Epoch = 1716 Step =   15440 loss = 0.018\n",
      "Epoch = 1717 Step =   15450 loss = 0.018\n",
      "Epoch = 1718 Step =   15460 loss = 0.018\n",
      "Epoch = 1719 Step =   15470 loss = 0.018\n",
      "Epoch = 1720 Step =   15480 loss = 0.018\n",
      "Epoch = 1722 Step =   15490 loss = 0.018\n",
      "Epoch = 1723 Step =   15500 loss = 0.018\n",
      "Epoch = 1724 Step =   15510 loss = 0.018\n",
      "Epoch = 1725 Step =   15520 loss = 0.018\n",
      "Epoch = 1726 Step =   15530 loss = 0.018\n",
      "Epoch = 1727 Step =   15540 loss = 0.018\n",
      "Epoch = 1728 Step =   15550 loss = 0.018\n",
      "Epoch = 1729 Step =   15560 loss = 0.018\n",
      "Epoch = 1730 Step =   15570 loss = 0.018\n",
      "Epoch = 1732 Step =   15580 loss = 0.018\n",
      "Epoch = 1733 Step =   15590 loss = 0.018\n",
      "Epoch = 1734 Step =   15600 loss = 0.018\n",
      "Epoch = 1735 Step =   15610 loss = 0.018\n",
      "Epoch = 1736 Step =   15620 loss = 0.018\n",
      "Epoch = 1737 Step =   15630 loss = 0.018\n",
      "Epoch = 1738 Step =   15640 loss = 0.018\n",
      "Epoch = 1739 Step =   15650 loss = 0.018\n",
      "Epoch = 1740 Step =   15660 loss = 0.018\n",
      "Epoch = 1742 Step =   15670 loss = 0.018\n",
      "Epoch = 1743 Step =   15680 loss = 0.018\n",
      "Epoch = 1744 Step =   15690 loss = 0.018\n",
      "Epoch = 1745 Step =   15700 loss = 0.018\n",
      "Epoch = 1746 Step =   15710 loss = 0.018\n",
      "Epoch = 1747 Step =   15720 loss = 0.018\n",
      "Epoch = 1748 Step =   15730 loss = 0.018\n",
      "Epoch = 1749 Step =   15740 loss = 0.018\n",
      "Epoch = 1750 Step =   15750 loss = 0.018\n",
      "Epoch = 1752 Step =   15760 loss = 0.018\n",
      "Epoch = 1753 Step =   15770 loss = 0.018\n",
      "Epoch = 1754 Step =   15780 loss = 0.018\n",
      "Epoch = 1755 Step =   15790 loss = 0.018\n",
      "Epoch = 1756 Step =   15800 loss = 0.018\n",
      "Epoch = 1757 Step =   15810 loss = 0.018\n",
      "Epoch = 1758 Step =   15820 loss = 0.018\n",
      "Epoch = 1759 Step =   15830 loss = 0.018\n",
      "Epoch = 1760 Step =   15840 loss = 0.018\n",
      "Epoch = 1762 Step =   15850 loss = 0.017\n",
      "Epoch = 1763 Step =   15860 loss = 0.017\n",
      "Epoch = 1764 Step =   15870 loss = 0.017\n",
      "Epoch = 1765 Step =   15880 loss = 0.017\n",
      "Epoch = 1766 Step =   15890 loss = 0.017\n",
      "Epoch = 1767 Step =   15900 loss = 0.017\n",
      "Epoch = 1768 Step =   15910 loss = 0.017\n",
      "Epoch = 1769 Step =   15920 loss = 0.017\n",
      "Epoch = 1770 Step =   15930 loss = 0.017\n",
      "Epoch = 1772 Step =   15940 loss = 0.017\n",
      "Epoch = 1773 Step =   15950 loss = 0.017\n",
      "Epoch = 1774 Step =   15960 loss = 0.017\n",
      "Epoch = 1775 Step =   15970 loss = 0.017\n",
      "Epoch = 1776 Step =   15980 loss = 0.017\n",
      "Epoch = 1777 Step =   15990 loss = 0.017\n",
      "Epoch = 1778 Step =   16000 loss = 0.017\n",
      "Epoch = 1779 Step =   16010 loss = 0.017\n",
      "Epoch = 1780 Step =   16020 loss = 0.017\n",
      "Epoch = 1782 Step =   16030 loss = 0.017\n",
      "Epoch = 1783 Step =   16040 loss = 0.017\n",
      "Epoch = 1784 Step =   16050 loss = 0.017\n",
      "Epoch = 1785 Step =   16060 loss = 0.017\n",
      "Epoch = 1786 Step =   16070 loss = 0.017\n",
      "Epoch = 1787 Step =   16080 loss = 0.017\n",
      "Epoch = 1788 Step =   16090 loss = 0.017\n",
      "Epoch = 1789 Step =   16100 loss = 0.017\n",
      "Epoch = 1790 Step =   16110 loss = 0.017\n",
      "Epoch = 1792 Step =   16120 loss = 0.017\n",
      "Epoch = 1793 Step =   16130 loss = 0.017\n",
      "Epoch = 1794 Step =   16140 loss = 0.017\n",
      "Epoch = 1795 Step =   16150 loss = 0.017\n",
      "Epoch = 1796 Step =   16160 loss = 0.017\n",
      "Epoch = 1797 Step =   16170 loss = 0.017\n",
      "Epoch = 1798 Step =   16180 loss = 0.017\n",
      "Epoch = 1799 Step =   16190 loss = 0.017\n",
      "Epoch = 1800 Step =   16200 loss = 0.017\n",
      "Epoch = 1802 Step =   16210 loss = 0.017\n",
      "Epoch = 1803 Step =   16220 loss = 0.017\n",
      "Epoch = 1804 Step =   16230 loss = 0.017\n",
      "Epoch = 1805 Step =   16240 loss = 0.017\n",
      "Epoch = 1806 Step =   16250 loss = 0.017\n",
      "Epoch = 1807 Step =   16260 loss = 0.017\n",
      "Epoch = 1808 Step =   16270 loss = 0.017\n",
      "Epoch = 1809 Step =   16280 loss = 0.017\n",
      "Epoch = 1810 Step =   16290 loss = 0.017\n",
      "Epoch = 1812 Step =   16300 loss = 0.017\n",
      "Epoch = 1813 Step =   16310 loss = 0.017\n",
      "Epoch = 1814 Step =   16320 loss = 0.017\n",
      "Epoch = 1815 Step =   16330 loss = 0.017\n",
      "Epoch = 1816 Step =   16340 loss = 0.017\n",
      "Epoch = 1817 Step =   16350 loss = 0.017\n",
      "Epoch = 1818 Step =   16360 loss = 0.017\n",
      "Epoch = 1819 Step =   16370 loss = 0.017\n",
      "Epoch = 1820 Step =   16380 loss = 0.017\n",
      "Epoch = 1822 Step =   16390 loss = 0.017\n",
      "Epoch = 1823 Step =   16400 loss = 0.017\n",
      "Epoch = 1824 Step =   16410 loss = 0.017\n",
      "Epoch = 1825 Step =   16420 loss = 0.017\n",
      "Epoch = 1826 Step =   16430 loss = 0.017\n",
      "Epoch = 1827 Step =   16440 loss = 0.017\n",
      "Epoch = 1828 Step =   16450 loss = 0.017\n",
      "Epoch = 1829 Step =   16460 loss = 0.017\n",
      "Epoch = 1830 Step =   16470 loss = 0.017\n",
      "Epoch = 1832 Step =   16480 loss = 0.017\n",
      "Epoch = 1833 Step =   16490 loss = 0.017\n",
      "Epoch = 1834 Step =   16500 loss = 0.017\n",
      "Epoch = 1835 Step =   16510 loss = 0.017\n",
      "Epoch = 1836 Step =   16520 loss = 0.017\n",
      "Epoch = 1837 Step =   16530 loss = 0.017\n",
      "Epoch = 1838 Step =   16540 loss = 0.017\n",
      "Epoch = 1839 Step =   16550 loss = 0.017\n",
      "Epoch = 1840 Step =   16560 loss = 0.017\n",
      "Epoch = 1842 Step =   16570 loss = 0.017\n",
      "Epoch = 1843 Step =   16580 loss = 0.017\n",
      "Epoch = 1844 Step =   16590 loss = 0.017\n",
      "Epoch = 1845 Step =   16600 loss = 0.017\n",
      "Epoch = 1846 Step =   16610 loss = 0.017\n",
      "INFO:tensorflow:model/global_step/sec: 17.2339\n",
      "Epoch = 1847 Step =   16620 loss = 0.017\n",
      "Epoch = 1848 Step =   16630 loss = 0.017\n",
      "Epoch = 1849 Step =   16640 loss = 0.017\n",
      "Epoch = 1850 Step =   16650 loss = 0.017\n",
      "Epoch = 1852 Step =   16660 loss = 0.017\n",
      "Epoch = 1853 Step =   16670 loss = 0.017\n",
      "Epoch = 1854 Step =   16680 loss = 0.017\n",
      "Epoch = 1855 Step =   16690 loss = 0.017\n",
      "Epoch = 1856 Step =   16700 loss = 0.017\n",
      "Epoch = 1857 Step =   16710 loss = 0.017\n",
      "Epoch = 1858 Step =   16720 loss = 0.017\n",
      "Epoch = 1859 Step =   16730 loss = 0.017\n",
      "Epoch = 1860 Step =   16740 loss = 0.017\n",
      "Epoch = 1862 Step =   16750 loss = 0.017\n",
      "Epoch = 1863 Step =   16760 loss = 0.017\n",
      "Epoch = 1864 Step =   16770 loss = 0.017\n",
      "Epoch = 1865 Step =   16780 loss = 0.017\n",
      "Epoch = 1866 Step =   16790 loss = 0.017\n",
      "Epoch = 1867 Step =   16800 loss = 0.017\n",
      "Epoch = 1868 Step =   16810 loss = 0.017\n",
      "Epoch = 1869 Step =   16820 loss = 0.017\n",
      "Epoch = 1870 Step =   16830 loss = 0.017\n",
      "Epoch = 1872 Step =   16840 loss = 0.017\n",
      "Epoch = 1873 Step =   16850 loss = 0.017\n",
      "Epoch = 1874 Step =   16860 loss = 0.017\n",
      "Epoch = 1875 Step =   16870 loss = 0.017\n",
      "Epoch = 1876 Step =   16880 loss = 0.017\n",
      "Epoch = 1877 Step =   16890 loss = 0.017\n",
      "Epoch = 1878 Step =   16900 loss = 0.017\n",
      "Epoch = 1879 Step =   16910 loss = 0.017\n",
      "Epoch = 1880 Step =   16920 loss = 0.017\n",
      "Epoch = 1882 Step =   16930 loss = 0.016\n",
      "Epoch = 1883 Step =   16940 loss = 0.016\n",
      "Epoch = 1884 Step =   16950 loss = 0.016\n",
      "Epoch = 1885 Step =   16960 loss = 0.016\n",
      "Epoch = 1886 Step =   16970 loss = 0.016\n",
      "Epoch = 1887 Step =   16980 loss = 0.016\n",
      "Epoch = 1888 Step =   16990 loss = 0.016\n",
      "Epoch = 1889 Step =   17000 loss = 0.016\n",
      "Epoch = 1890 Step =   17010 loss = 0.016\n",
      "Epoch = 1892 Step =   17020 loss = 0.016\n",
      "Epoch = 1893 Step =   17030 loss = 0.016\n",
      "Epoch = 1894 Step =   17040 loss = 0.016\n",
      "Epoch = 1895 Step =   17050 loss = 0.016\n",
      "Epoch = 1896 Step =   17060 loss = 0.016\n",
      "Epoch = 1897 Step =   17070 loss = 0.016\n",
      "Epoch = 1898 Step =   17080 loss = 0.016\n",
      "Epoch = 1899 Step =   17090 loss = 0.016\n",
      "Epoch = 1900 Step =   17100 loss = 0.016\n",
      "Epoch = 1902 Step =   17110 loss = 0.016\n",
      "Epoch = 1903 Step =   17120 loss = 0.016\n",
      "Epoch = 1904 Step =   17130 loss = 0.016\n",
      "Epoch = 1905 Step =   17140 loss = 0.016\n",
      "Epoch = 1906 Step =   17150 loss = 0.016\n",
      "Epoch = 1907 Step =   17160 loss = 0.016\n",
      "Epoch = 1908 Step =   17170 loss = 0.016\n",
      "Epoch = 1909 Step =   17180 loss = 0.016\n",
      "Epoch = 1910 Step =   17190 loss = 0.016\n",
      "Epoch = 1912 Step =   17200 loss = 0.016\n",
      "Epoch = 1913 Step =   17210 loss = 0.016\n",
      "Epoch = 1914 Step =   17220 loss = 0.016\n",
      "Epoch = 1915 Step =   17230 loss = 0.016\n",
      "Epoch = 1916 Step =   17240 loss = 0.016\n",
      "Epoch = 1917 Step =   17250 loss = 0.016\n",
      "Epoch = 1918 Step =   17260 loss = 0.016\n",
      "Epoch = 1919 Step =   17270 loss = 0.016\n",
      "Epoch = 1920 Step =   17280 loss = 0.016\n",
      "Epoch = 1922 Step =   17290 loss = 0.016\n",
      "Epoch = 1923 Step =   17300 loss = 0.016\n",
      "Epoch = 1924 Step =   17310 loss = 0.016\n",
      "Epoch = 1925 Step =   17320 loss = 0.016\n",
      "Epoch = 1926 Step =   17330 loss = 0.016\n",
      "Epoch = 1927 Step =   17340 loss = 0.016\n",
      "Epoch = 1928 Step =   17350 loss = 0.016\n",
      "Epoch = 1929 Step =   17360 loss = 0.016\n",
      "Epoch = 1930 Step =   17370 loss = 0.016\n",
      "Epoch = 1932 Step =   17380 loss = 0.016\n",
      "Epoch = 1933 Step =   17390 loss = 0.016\n",
      "Epoch = 1934 Step =   17400 loss = 0.016\n",
      "Epoch = 1935 Step =   17410 loss = 0.016\n",
      "Epoch = 1936 Step =   17420 loss = 0.016\n",
      "Epoch = 1937 Step =   17430 loss = 0.016\n",
      "Epoch = 1938 Step =   17440 loss = 0.016\n",
      "Epoch = 1939 Step =   17450 loss = 0.016\n",
      "Epoch = 1940 Step =   17460 loss = 0.016\n",
      "Epoch = 1942 Step =   17470 loss = 0.016\n",
      "Epoch = 1943 Step =   17480 loss = 0.016\n",
      "Epoch = 1944 Step =   17490 loss = 0.016\n",
      "Epoch = 1945 Step =   17500 loss = 0.016\n",
      "Epoch = 1946 Step =   17510 loss = 0.016\n",
      "Epoch = 1947 Step =   17520 loss = 0.016\n",
      "Epoch = 1948 Step =   17530 loss = 0.016\n",
      "Epoch = 1949 Step =   17540 loss = 0.016\n",
      "Epoch = 1950 Step =   17550 loss = 0.016\n",
      "Epoch = 1952 Step =   17560 loss = 0.016\n",
      "Epoch = 1953 Step =   17570 loss = 0.016\n",
      "Epoch = 1954 Step =   17580 loss = 0.016\n",
      "Epoch = 1955 Step =   17590 loss = 0.016\n",
      "Epoch = 1956 Step =   17600 loss = 0.016\n",
      "Epoch = 1957 Step =   17610 loss = 0.016\n",
      "Epoch = 1958 Step =   17620 loss = 0.016\n",
      "Epoch = 1959 Step =   17630 loss = 0.016\n",
      "Epoch = 1960 Step =   17640 loss = 0.016\n",
      "Epoch = 1962 Step =   17650 loss = 0.016\n",
      "Epoch = 1963 Step =   17660 loss = 0.016\n",
      "Epoch = 1964 Step =   17670 loss = 0.016\n",
      "Epoch = 1965 Step =   17680 loss = 0.016\n",
      "Epoch = 1966 Step =   17690 loss = 0.016\n",
      "Epoch = 1967 Step =   17700 loss = 0.016\n",
      "Epoch = 1968 Step =   17710 loss = 0.016\n",
      "Epoch = 1969 Step =   17720 loss = 0.016\n",
      "Epoch = 1970 Step =   17730 loss = 0.016\n",
      "Epoch = 1972 Step =   17740 loss = 0.016\n",
      "Epoch = 1973 Step =   17750 loss = 0.016\n",
      "Epoch = 1974 Step =   17760 loss = 0.016\n",
      "Epoch = 1975 Step =   17770 loss = 0.016\n",
      "Epoch = 1976 Step =   17780 loss = 0.016\n",
      "Epoch = 1977 Step =   17790 loss = 0.016\n",
      "Epoch = 1978 Step =   17800 loss = 0.016\n",
      "Epoch = 1979 Step =   17810 loss = 0.016\n",
      "Epoch = 1980 Step =   17820 loss = 0.016\n",
      "Epoch = 1982 Step =   17830 loss = 0.016\n",
      "Epoch = 1983 Step =   17840 loss = 0.016\n",
      "Epoch = 1984 Step =   17850 loss = 0.016\n",
      "Epoch = 1985 Step =   17860 loss = 0.016\n",
      "Epoch = 1986 Step =   17870 loss = 0.016\n",
      "Epoch = 1987 Step =   17880 loss = 0.016\n",
      "Epoch = 1988 Step =   17890 loss = 0.016\n",
      "Epoch = 1989 Step =   17900 loss = 0.016\n",
      "Epoch = 1990 Step =   17910 loss = 0.016\n",
      "Epoch = 1992 Step =   17920 loss = 0.016\n",
      "Epoch = 1993 Step =   17930 loss = 0.016\n",
      "Epoch = 1994 Step =   17940 loss = 0.016\n",
      "Epoch = 1995 Step =   17950 loss = 0.016\n",
      "Epoch = 1996 Step =   17960 loss = 0.016\n",
      "Epoch = 1997 Step =   17970 loss = 0.016\n",
      "Epoch = 1998 Step =   17980 loss = 0.016\n",
      "Epoch = 1999 Step =   17990 loss = 0.016\n",
      "Epoch = 2000 Step =   18000 loss = 0.016\n",
      "Training is done.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_models/model.ckpt-10398\n",
      "INFO:tensorflow:Froze 7 variables.\n",
      "INFO:tensorflow:Converted 7 variables to const ops.\n",
      "2066 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "#run train()\n",
    "train(train_id_data, num_vocabs, num_target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_pred = './data/pred_data.txt'\n",
    "\n",
    "def pred_data(path):\n",
    "    sent = []\n",
    "    \n",
    "    with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n\\r')\n",
    "            sent.append(line)\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = pred_data(fn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_target: POS pred_probs: 0.839555\n",
      "pred_target: DES pred_probs: 1.0\n",
      "pred_target: DES pred_probs: 0.999999\n",
      "pred_target: DES pred_probs: 0.997163\n",
      "pred_target: DES pred_probs: 1.0\n",
      "pred_target: DES pred_probs: 1.0\n",
      "pred_target: DES pred_probs: 0.999999\n",
      "pred_target: DES pred_probs: 1.0\n",
      "pred_target: DES pred_probs: 0.999999\n",
      "pred_target: POS pred_probs: 0.999996\n",
      "pred_target: POS pred_probs: 0.999561\n",
      "pred_target: DES pred_probs: 1.0\n",
      "pred_target: POS pred_probs: 0.999982\n",
      "pred_target: POS pred_probs: 0.999968\n",
      "pred_target: NEG pred_probs: 0.999993\n",
      "pred_target: POS pred_probs: 0.999999\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for sent in sents:\n",
    "    results.append(predict(token_vocab, target_vocab, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_predict: 16\n",
      "score: 11, average: 0.6875\n"
     ]
    }
   ],
   "source": [
    "pred_label = []\n",
    "\n",
    "with codecs.open('./data/pred_label.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n\\r')\n",
    "        pred_label.append(line)\n",
    "\n",
    "\n",
    "score = 0\n",
    "for i in range(len(results)):\n",
    "    if pred_label[i] == results[i][0]:\n",
    "        score += 1\n",
    "\n",
    "average_score = score / len(results)\n",
    "print('num_predict:', len(results))\n",
    "print('score: {}, average: {}'.format(score, average_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
